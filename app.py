import os
import streamlit as st
# Thay ƒë·ªïi: Import ChatGoogleGenerativeAI thay v√¨ ChatGroq
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from streamlit_option_menu import option_menu

from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate # Th√™m import n√†y cho Gemini

load_dotenv()

# --- C·∫§U H√åNH TRANG STREAMLIT ---
st.set_page_config(page_title="ü§ñ Chatbot AI", layout="wide")

# --- Kh·ªüi t·∫°o GenAI client ---
genai_api_key = os.getenv('GOOGLE_API_KEY')
if not genai_api_key:
    st.error("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Gemini.")
    st.stop()

genai.configure(api_key=genai_api_key)

chat = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    google_api_key=genai_api_key,
    convert_system_message_to_human=True
)

# Template ch√≠nh cho RAG (s·ª≠ d·ª•ng c·∫£ context v√† input)
# ƒêi·ªÅu ch·ªânh prompt template m·ªôt ch√∫t cho ph√π h·ª£p v·ªõi c√°ch Gemini x·ª≠ l√Ω system prompt h∆°n
PROMPT_TEMPLATE = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.
S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.

Ng·ªØ c·∫£nh:
{context}
"""
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", PROMPT_TEMPLATE),
    ("human", "{input}")
])


# --- Kh·ªüi t·∫°o Session State Flags cho th√¥ng b√°o ---
if 'initial_embed_toast_shown' not in st.session_state:
    st.session_state.initial_embed_toast_shown = False
if 'initial_embed_error_toast_shown' not in st.session_state:
    st.session_state.initial_embed_error_toast_shown = False
if 'initial_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_loaded_toast_shown = False
if 'initial_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_not_found_toast_shown = False
if 'initial_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_error_toast_shown = False
if 'initial_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_faiss_load_attempted = False


# --- ƒê·∫∂T ƒê∆Ø·ªúNG D·∫™N C·ª§C B·ªò ---
current_script_directory = os.path.dirname(os.path.abspath(__file__))
local_embedding_model_path = os.path.join(current_script_directory, "local_models", "multilingual-e5-large")
REMOTE_EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
# REMOTE_EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"


# --- H√†m t·∫£i Embedding Model (CH·ªà H√ÄM N√ÄY D√ôNG @st.cache_resource) ---
@st.cache_resource
def get_huggingface_embeddings_model(model_path: str, is_local: bool):
    try:
        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa model c·ª•c b·ªô n·∫øu is_local l√† True
        if is_local and not (os.path.exists(model_path) and
                             (os.path.exists(os.path.join(model_path, 'pytorch_model.bin')) or
                              os.path.exists(os.path.join(model_path, 'model.safetensors')) or
                              os.path.exists(os.path.join(model_path, 'config.json')))):
            return None, "load_error:Local model path provided but no valid model files found."

        embed_model = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'} # C√≥ th·ªÉ ƒë·ªïi th√†nh 'cuda' n·∫øu c√≥ GPU
        )
        return embed_model, "loaded_successfully"
    except Exception as e:
        return None, f"load_error:{e}"

# --- Logic t·∫£i m√¥ h√¨nh Embedding khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng ---
if "embeddings_object" not in st.session_state or st.session_state.embeddings_object is None:
    # 1. Th·ª≠ t·∫£i t·ª´ c·ª•c b·ªô tr∆∞·ªõc
    with st.spinner(f"ƒêang ki·ªÉm tra v√† t·∫£i m√¥ h√¨nh Embedding t·ª´ c·ª•c b·ªô ({local_embedding_model_path})..."):
        embed_model_result, status = get_huggingface_embeddings_model(local_embedding_model_path, is_local=True)

    if embed_model_result:
        st.session_state.embeddings_object = embed_model_result
        if not st.session_state.initial_embed_toast_shown:
            st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ c·ª•c b·ªô!", icon="‚úÖ")
            st.session_state.initial_embed_toast_shown = True
    else:
        # 2. Th·ª≠ t·∫£i t·ª´ Internet n·∫øu c·ª•c b·ªô kh√¥ng th√†nh c√¥ng
        with st.spinner(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh c·ª•c b·ªô. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})..."):
            st.toast(f"M√¥ h√¨nh Embedding c·ª•c b·ªô kh√¥ng t√¨m th·∫•y. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})...", icon="üåê")
            embed_model_result, status = get_huggingface_embeddings_model(REMOTE_EMBEDDING_MODEL_NAME, is_local=False)

        if embed_model_result:
            st.session_state.embeddings_object = embed_model_result
            if not st.session_state.initial_embed_toast_shown:
                st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ Internet!", icon="‚úÖ")
                st.session_state.initial_embed_toast_shown = True
        else:
            # N·∫øu c·∫£ hai c√°ch ƒë·ªÅu l·ªói
            if not st.session_state.initial_embed_error_toast_shown:
                error_message = status.split(":", 1)[1] if ":" in status else status
                st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Embedding: {error_message}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Internet ho·∫∑c ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh.")
                st.session_state.initial_embed_error_toast_shown = True
            st.stop()

embeddings = st.session_state.embeddings_object # L·∫•y ƒë·ªëi t∆∞·ª£ng embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i v√† cache

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u FAISS index ---
FAISS_PATH = "faiss_index_data_multilingual"
if not os.path.exists(FAISS_PATH):
    os.makedirs(FAISS_PATH)

# --- Qu·∫£n l√Ω FAISS Vector Store trong session state ---
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "processing_done" not in st.session_state:
    st.session_state.processing_done = False


# --- H√†m t·∫£i FAISS index (KH√îNG D√ôNG @st.cache_resource) ---
# H√†m n√†y ch·ªâ ch·ª©a logic t·∫£i/ki·ªÉm tra, kh√¥ng c√≥ l·ªánh Streamlit UI
def load_faiss_index_from_disk(path, embeddings_obj):
    if embeddings_obj is None: # ƒê·∫£m b·∫£o embeddings ƒë√£ s·∫µn s√†ng
        return None, "embeddings_not_ready"

    if os.path.exists(path) and os.listdir(path):
        try:
            vector_store = FAISS.load_local(
                path,
                embeddings_obj, # S·ª≠ d·ª•ng ƒë·ªëi t∆∞·ª£ng embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i v√† cache
                allow_dangerous_deserialization=True
            )
            return vector_store, "loaded_successfully"
        except Exception as e:
            return None, f"load_error:{e}"
    else:
        return None, "not_found"


# --- Logic t·∫£i FAISS ban ƒë·∫ßu (ch·ªâ ch·∫°y m·ªôt l·∫ßn sau khi embeddings c√≥ s·∫µn) ---
# Ch·ªâ c·ªë g·∫Øng t·∫£i FAISS t·ª´ ƒëƒ©a n·∫øu n√≥ ch∆∞a ƒë∆∞·ª£c t·∫£i v√†o session_state V√Ä ch∆∞a t·ª´ng th·ª≠ t·∫£i
if st.session_state.vector_store is None and not st.session_state.initial_faiss_load_attempted:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu AI t·ª´ FAISS index ƒë√£ l∆∞u trong '{FAISS_PATH}'..."):
        st.session_state.vector_store, status = load_faiss_index_from_disk(FAISS_PATH, embeddings) # Truy·ªÅn embeddings ƒë√£ ƒë∆∞·ª£c cache

    if status == "loaded_successfully":
        if not st.session_state.initial_faiss_loaded_toast_shown:
            st.toast(f"FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ th∆∞ m·ª•c '{FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_faiss_loaded_toast_shown = True
        st.session_state.processing_done = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_faiss_error_toast_shown:
            st.error(f"L·ªói khi t·∫£i FAISS index t·ª´ c·ª•c b·ªô: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_faiss_error_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_faiss_not_found_toast_shown:
            st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index trong th∆∞ m·ª•c '{FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_faiss_not_found_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "embeddings_not_ready":
        # ƒêi·ªÅu n√†y s·∫Ω kh√¥ng x·∫£y ra n·∫øu logic t·∫£i embeddings ch·∫°y tr∆∞·ªõc
        pass
    st.session_state.initial_faiss_load_attempted = True # ƒê√°nh d·∫•u l√† ƒë√£ th·ª≠ t·∫£i l·∫ßn ƒë·∫ßu


# --- Menu ƒëi·ªÅu h∆∞·ªõng d·ªçc ·ªü Sidebar ---
with st.sidebar:
    selected = option_menu(
        menu_title=None,
        options=["Chatbot", "Qu·∫£n l√Ω d·ªØ li·ªáu"],
        icons=["chat", "file-earmark-text"],
        menu_icon="list",
        default_index=0,
        orientation="vertical",
        styles={
            "container": {"padding": "0!important", "background-color": "transparent"},
            "icon": {"color": "orange", "font-size": "16px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#2ca8f2"},
        }
    )

# --- Logic hi·ªÉn th·ªã n·ªôi dung d·ª±a tr√™n l·ª±a ch·ªçn menu ---
if selected == "Chatbot":
    # Thay ƒë·ªïi ti√™u ƒë·ªÅ chatbot t·ª´ Grok sang Gemini
    st.title("ü§ñ Chatbot Gemini")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                if st.session_state.vector_store: # K√≠ch ho·∫°t RAG n·∫øu c√≥ vector_store
                    retriever = st.session_state.vector_store.as_retriever()
                    # S·ª≠ d·ª•ng rag_prompt (ChatPromptTemplate) ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n
                    combine_docs_chain = create_stuff_documents_chain(chat, rag_prompt)
                    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

                    response = retrieval_chain.invoke({"input": prompt})
                    answer = response["answer"]

                else:
                    # Fallback n·∫øu kh√¥ng c√≥ vector_store (chatbot t·ªïng qu√°t)
                    st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu c√° nh√¢n. Chatbot s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                    # Template ƒë∆°n gi·∫£n h∆°n, ch·ªâ c·∫ßn "input" khi kh√¥ng c√≥ RAG
                    fallback_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† t·ª± nhi√™n:
C√¢u h·ªèi: {input}
"""
                    # S·ª≠ d·ª•ng ChatPromptTemplate cho fallback c≈©ng v·∫≠y
                    default_prompt = ChatPromptTemplate.from_messages([
                        ("system", fallback_prompt_template),
                        ("human", "{input}")
                    ])
                    chain = default_prompt | chat
                    response = chain.invoke({"input": prompt})
                    answer = response.content

                with st.chat_message("assistant"):
                    st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

elif selected == "Qu·∫£n l√Ω d·ªØ li·ªáu":
    st.header("Trang Qu·∫£n l√Ω D·ªØ li·ªáu")
    st.write("T·∫°i ƒë√¢y, b·∫°n c√≥ th·ªÉ t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho chatbot ho·∫∑c xem tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán c√≥.")

    if embeddings is None:
        st.warning("M√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt.")

    uploaded_file = st.file_uploader(
        "Ch·ªçn m·ªôt file PDF, TXT, DOCX ho·∫∑c XLSX ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu",
        type=["pdf", "txt", "docx", "xlsx"],
        accept_multiple_files=False,
        help="T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ƒë·ªÉ chatbot c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ƒë√≥. Vi·ªác t·∫£i file m·ªõi s·∫Ω ghi ƒë√® d·ªØ li·ªáu c≈©."
    )

    if uploaded_file:
        if embeddings is None:
            st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√¨ m√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói tr√™n c√πng.")
        else:
            # T·∫°o m·ªôt th∆∞ m·ª•c t·∫°m th·ªùi ƒë·ªÉ l∆∞u file
            import tempfile
            import shutil
            temp_dir = tempfile.mkdtemp()
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)

            with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
                try:
                    file_extension = uploaded_file.name.split(".")[-1].lower()

                    with open(temp_file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())

                    docs = []
                    if file_extension == "pdf":
                        loader = PyPDFLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "txt":
                        loader = TextLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "docx":
                        loader = UnstructuredWordDocumentLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "xlsx":
                        loader = UnstructuredExcelLoader(temp_file_path)
                        docs = loader.load()
                    else:
                        st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i l√™n file PDF, TXT, DOCX, ho·∫∑c XLSX.")
                        shutil.rmtree(temp_dir)
                        st.stop()

                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200
                    )
                    splits = text_splitter.split_documents(docs)

                    # Lu√¥n t·∫°o m·ªõi ho·∫∑c ghi ƒë√® FAISS index khi t·∫£i file m·ªõi
                    st.session_state.vector_store = FAISS.from_documents(splits, embeddings)
                    st.session_state.vector_store.save_local(FAISS_PATH)

                    st.success(f"T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† FAISS index ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c '{FAISS_PATH}'.")

                    # Reset c√°c c·ªù tr·∫°ng th√°i ƒë·ªÉ th√¥ng b√°o FAISS c√≥ th·ªÉ hi·ªÉn th·ªã l·∫°i n·∫øu c·∫ßn
                    st.session_state.initial_faiss_loaded_toast_shown = False
                    st.session_state.initial_faiss_not_found_toast_shown = False
                    st.session_state.initial_faiss_error_toast_shown = False
                    st.session_state.initial_faiss_load_attempted = False # Bu·ªôc t·∫£i l·∫°i FAISS t·ª´ ƒëƒ©a n·∫øu trang ƒë∆∞·ª£c l√†m m·ªõi

                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}. Vui l√≤ng ki·ªÉm tra file ho·∫∑c c√†i ƒë·∫∑t th∆∞ vi·ªán 'unstructured'.")
                finally:
                    # D·ªçn d·∫πp: X√≥a th∆∞ m·ª•c t·∫°m th·ªùi
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)


    st.markdown("---")
    st.subheader("Tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán t·∫°i:")
    if st.session_state.vector_store:
        st.write("‚úÖ D·ªØ li·ªáu ri√™ng t∆∞ ƒë√£ ƒë∆∞·ª£c t·∫£i v√† s·∫µn s√†ng s·ª≠ d·ª•ng trong Chatbot.")
    else:
        st.write("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ri√™ng t∆∞ n√†o ƒë∆∞·ª£c t·∫£i. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.")

    if st.button("X√≥a FAISS Index hi·ªán c√≥"):
        if os.path.exists(FAISS_PATH):
            import shutil
            try:
                shutil.rmtree(FAISS_PATH)
                st.session_state.vector_store = None
                # Reset t·∫•t c·∫£ c√°c c·ªù tr·∫°ng th√°i li√™n quan ƒë·∫øn FAISS ƒë·ªÉ th√¥ng b√°o hi·ªÉn th·ªã l·∫°i
                st.session_state.initial_faiss_loaded_toast_shown = False
                st.session_state.initial_faiss_not_found_toast_shown = False
                st.session_state.initial_faiss_error_toast_shown = False
                st.session_state.initial_faiss_load_attempted = False # Bu·ªôc t·∫£i l·∫°i FAISS t·ª´ ƒëƒ©a l·∫ßn sau
                st.success("FAISS index ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng. Vui l√≤ng t·∫£i l·∫°i trang ho·∫∑c t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ t·∫°o l·∫°i.")
                st.rerun() # Y√™u c·∫ßu Streamlit ch·∫°y l·∫°i app ƒë·ªÉ c·∫≠p nh·∫≠t tr·∫°ng th√°i
            except Exception as e:
                st.error(f"L·ªói khi x√≥a FAISS index: {e}")
        else:
            st.info("Kh√¥ng t√¨m th·∫•y FAISS index ƒë·ªÉ x√≥a.")