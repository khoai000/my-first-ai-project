import os
import streamlit as st
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from streamlit_option_menu import option_menu

from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

load_dotenv()

# --- L·ªÜNH st.set_page_config() PH·∫¢I L√Ä L·ªÜNH STREAMLIT ƒê·∫¶U TI√äN TRONG SCRIPT! ---
st.set_page_config(page_title="ü§ñ Chatbot AI", layout="wide")


# --- Kh·ªüi t·∫°o Groq client ---
groq_api_key = os.getenv('GROQ_API_KEY')
if not groq_api_key:
    st.error("GROQ_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Groq.")
    st.stop()

chat = ChatGroq(
    temperature=0,
    groq_api_key=groq_api_key,
    model_name="llama3-70b-8192"
)

# Template ch√≠nh cho RAG (s·ª≠ d·ª•ng c·∫£ context v√† input)
PROMPT_TEMPLATE = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† t·ª± nhi√™n.
S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {input}
"""

# --- Kh·ªüi t·∫°o Session State Flags cho th√¥ng b√°o ---
if 'initial_embed_toast_shown' not in st.session_state:
    st.session_state.initial_embed_toast_shown = False
if 'initial_embed_error_toast_shown' not in st.session_state:
    st.session_state.initial_embed_error_toast_shown = False
if 'initial_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_loaded_toast_shown = False
if 'initial_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_not_found_toast_shown = False
if 'initial_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_error_toast_shown = False


# --- ƒê·∫∂T ƒê∆Ø·ªúNG D·∫™N C·ª§C B·ªò M·ªòT C√ÅCH V·ªÆNG CH·∫ÆC ·ªû ƒê√ÇY ---
current_script_directory = os.path.dirname(os.path.abspath(__file__))
local_embedding_model_path = os.path.join(current_script_directory, "local_models", "multilingual-e5-large")


# --- H√†m t·∫£i Embedding Model (ch·ªâ ch·ª©a logic t√≠nh to√°n, kh√¥ng c√≥ l·ªánh Streamlit UI) ---
@st.cache_resource
def _get_huggingface_embeddings_pure(local_model_path: str):
    try:
        embed_model = HuggingFaceEmbeddings(
            model_name=local_model_path,
            model_kwargs={'device': 'cpu'}
        )
        return embed_model, "loaded_successfully"
    except Exception as e:
        return None, f"load_error:{e}"

# --- Logic g·ªçi h√†m t·∫£i Embedding v√† x·ª≠ l√Ω UI d·ª±a tr√™n k·∫øt qu·∫£ ---
if "embeddings_object" not in st.session_state:
    st.session_state.embeddings_object = None

if st.session_state.embeddings_object is None:
    with st.spinner(f"ƒêang t·∫£i m√¥ h√¨nh Embedding t·ª´ '{os.path.basename(local_embedding_model_path)}'..."):
        embed_model_result, status = _get_huggingface_embeddings_pure(local_embedding_model_path)

    st.session_state.embeddings_object = embed_model_result

    if status == "loaded_successfully":
        if not st.session_state.initial_embed_toast_shown:
            st.toast(f"M√¥ h√¨nh Embedding t·ª´ '{os.path.basename(local_embedding_model_path)}' ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!", icon="‚úÖ")
            st.session_state.initial_embed_toast_shown = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_embed_error_toast_shown:
            st.error(f"L·ªói khi kh·ªüi t·∫°o HuggingFaceEmbeddings t·ª´ ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô '{local_embedding_model_path}': {error_message}. Vui l√≤ng ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i xu·ªëng ƒë·∫ßy ƒë·ªß v√† ƒë√∫ng v·ªã tr√≠ ch∆∞a.")
            st.session_state.initial_embed_error_toast_shown = True

embeddings = st.session_state.embeddings_object


# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u FAISS index ---
FAISS_PATH = "faiss_index_data_multilingual"
if not os.path.exists(FAISS_PATH):
    os.makedirs(FAISS_PATH)

# --- Qu·∫£n l√Ω FAISS Vector Store trong session state ---
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "processing_done" not in st.session_state:
    st.session_state.processing_done = False


# --- H√†m t·∫£i FAISS index (ch·ªâ ch·ª©a logic t√≠nh to√°n, kh√¥ng c√≥ l·ªánh Streamlit UI) ---
@st.cache_resource(hash_funcs={HuggingFaceEmbeddings: lambda _: None})
def get_faiss_index_pure(current_embeddings, path):
    """
    H√†m n√†y ch·ªâ th·ª±c hi·ªán vi·ªác t·∫£i FAISS index v√†o b·ªô nh·ªõ.
    Tuy·ªát ƒë·ªëi KH√îNG ch·ª©a c√°c l·ªánh Streamlit UI nh∆∞ st.spinner, st.toast, st.error.
    """
    if current_embeddings is None:
        return None, "embeddings_not_ready"

    if os.path.exists(path) and os.listdir(path):
        try:
            vector_store = FAISS.load_local(
                path,
                current_embeddings,
                allow_dangerous_deserialization=True
            )
            return vector_store, "loaded_successfully"
        except Exception as e:
            return None, f"load_error:{e}"
    else:
        return None, "not_found"


# --- Logic g·ªçi h√†m t·∫£i FAISS v√† x·ª≠ l√Ω UI d·ª±a tr√™n k·∫øt qu·∫£ ---
if st.session_state.vector_store is None:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu AI t·ª´ FAISS index ƒë√£ l∆∞u trong '{FAISS_PATH}'..."):
        st.session_state.vector_store, status = get_faiss_index_pure(embeddings, FAISS_PATH)

    if status == "loaded_successfully":
        if not st.session_state.initial_faiss_loaded_toast_shown:
            st.toast(f"FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ th∆∞ m·ª•c '{FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_faiss_loaded_toast_shown = True
        st.session_state.processing_done = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_faiss_error_toast_shown:
            st.error(f"L·ªói khi t·∫£i FAISS index t·ª´ c·ª•c b·ªô: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_faiss_error_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_faiss_not_found_toast_shown:
            st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index trong th∆∞ m·ª•c '{FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_faiss_not_found_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "embeddings_not_ready":
        pass


# --- Menu ƒëi·ªÅu h∆∞·ªõng d·ªçc ·ªü Sidebar ---
with st.sidebar:
    selected = option_menu(
        menu_title=None,
        options=["Chatbot", "Qu·∫£n l√Ω d·ªØ li·ªáu"],
        icons=["chat", "file-earmark-text"],
        menu_icon="list",
        default_index=0,
        orientation="vertical",
        styles={
            "container": {"padding": "0!important", "background-color": "transparent"},
            "icon": {"color": "orange", "font-size": "16px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#2ca8f2"},
        }
    )

# --- Logic hi·ªÉn th·ªã n·ªôi dung d·ª±a tr√™n l·ª±a ch·ªçn menu ---
if selected == "Chatbot":
    st.title("ü§ñ Chatbot Grok")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                if st.session_state.vector_store: # K√≠ch ho·∫°t RAG n·∫øu c√≥ vector_store
                    retriever = st.session_state.vector_store.as_retriever()
                    rag_prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                    combine_docs_chain = create_stuff_documents_chain(chat, rag_prompt)
                    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

                    response = retrieval_chain.invoke({"input": prompt})
                    answer = response["answer"]

                else:
                    # Fallback n·∫øu kh√¥ng c√≥ vector_store (chatbot t·ªïng qu√°t)
                    st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu c√° nh√¢n. Chatbot s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                    # Template ƒë∆°n gi·∫£n h∆°n, ch·ªâ c·∫ßn "input" khi kh√¥ng c√≥ RAG
                    fallback_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† t·ª± nhi√™n:
C√¢u h·ªèi: {input}
"""
                    default_prompt = PromptTemplate(
                        template=fallback_prompt_template, # S·ª≠ d·ª•ng template ri√™ng cho fallback
                        input_variables=["input"]
                    )
                    chain = default_prompt | chat
                    response = chain.invoke({"input": prompt})
                    answer = response.content

                with st.chat_message("assistant"):
                    st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

elif selected == "Qu·∫£n l√Ω d·ªØ li·ªáu":
    st.header("Trang Qu·∫£n l√Ω D·ªØ li·ªáu")
    st.write("T·∫°i ƒë√¢y, b·∫°n c√≥ th·ªÉ t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho chatbot ho·∫∑c xem tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán c√≥.")

    if embeddings is None:
        st.warning("M√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt.")

    uploaded_file = st.file_uploader(
        "Ch·ªçn m·ªôt file PDF, TXT, DOCX ho·∫∑c XLSX ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu",
        type=["pdf", "txt", "docx", "xlsx"],
        accept_multiple_files=False,
        help="T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ƒë·ªÉ chatbot c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ƒë√≥. Vi·ªác t·∫£i file m·ªõi s·∫Ω ghi ƒë√® d·ªØ li·ªáu c≈©."
    )

    if uploaded_file:
        if embeddings is None:
            st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√¨ m√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói tr√™n c√πng.")
        else:
            with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
                try:
                    file_extension = uploaded_file.name.split(".")[-1].lower()
                    temp_file_path = f"temp_uploaded_file.{file_extension}"
                    with open(temp_file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())

                    docs = []
                    if file_extension == "pdf":
                        loader = PyPDFLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "txt":
                        loader = TextLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "docx":
                        loader = UnstructuredWordDocumentLoader(temp_file_path)
                        docs = loader.load()
                    elif file_extension == "xlsx":
                        loader = UnstructuredExcelLoader(temp_file_path)
                        docs = loader.load()
                    else:
                        st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i l√™n file PDF, TXT, DOCX, ho·∫∑c XLSX.")
                        os.remove(temp_file_path)
                        st.stop()

                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200
                    )
                    splits = text_splitter.split_documents(docs)

                    st.session_state.vector_store = FAISS.from_documents(splits, embeddings)
                    st.session_state.vector_store.save_local(FAISS_PATH)

                    st.success(f"T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† FAISS index ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c '{FAISS_PATH}'.")

                    # Sau khi t·∫°o/c·∫≠p nh·∫≠t FAISS, x√≥a cache cho h√†m t·∫£i FAISS
                    get_faiss_index_pure.clear()
                    st.session_state.initial_faiss_loaded_toast_shown = False
                    st.session_state.initial_faiss_not_found_toast_shown = False
                    st.session_state.initial_faiss_error_toast_shown = False

                    if os.path.exists(temp_file_path):
                        os.remove(temp_file_path)

                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}. Vui l√≤ng ki·ªÉm tra file ho·∫∑c c√†i ƒë·∫∑t th∆∞ vi·ªán 'unstructured'.")

    st.markdown("---")
    st.subheader("Tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán t·∫°i:")
    if st.session_state.vector_store:
        st.write("‚úÖ D·ªØ li·ªáu ri√™ng t∆∞ ƒë√£ ƒë∆∞·ª£c t·∫£i v√† s·∫µn s√†ng s·ª≠ d·ª•ng trong Chatbot.")
    else:
        st.write("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ri√™ng t∆∞ n√†o ƒë∆∞·ª£c t·∫£i. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.")