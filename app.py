import os
import streamlit as st
# Thay ƒë·ªïi: Import ChatGoogleGenerativeAI thay v√¨ ChatGroq
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from streamlit_option_menu import option_menu

from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate # Th√™m import n√†y cho Gemini

load_dotenv()

# --- C·∫§U H√åNH TRANG STREAMLIT ---
st.set_page_config(page_title="ü§ñ Chatbot AI", layout="wide")

# --- Kh·ªüi t·∫°o GenAI client ---
genai_api_key = os.getenv('GOOGLE_API_KEY')
if not genai_api_key:
    st.error("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Gemini.")
    st.stop()

genai.configure(api_key=genai_api_key)

chat = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    google_api_key=genai_api_key,
    convert_system_message_to_human=True
)

# Template ch√≠nh cho RAG (s·ª≠ d·ª•ng c·∫£ context v√† input)
PROMPT_TEMPLATE = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.
S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.

Ng·ªØ c·∫£nh:
{context}
"""
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", PROMPT_TEMPLATE),
    ("human", "{input}")
])


# --- Kh·ªüi t·∫°o Session State Flags cho th√¥ng b√°o ---
if 'initial_embed_toast_shown' not in st.session_state:
    st.session_state.initial_embed_toast_shown = False
if 'initial_embed_error_toast_shown' not in st.session_state:
    st.session_state.initial_embed_error_toast_shown = False
if 'initial_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_loaded_toast_shown = False
if 'initial_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_not_found_toast_shown = False
if 'initial_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_error_toast_shown = False
if 'initial_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_faiss_load_attempted = False

# C·ªù cho Base Document FAISS (M·ªöI)
if 'initial_base_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_base_faiss_loaded_toast_shown = False
if 'initial_base_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_base_faiss_not_found_toast_shown = False
if 'initial_base_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_base_faiss_error_toast_shown = False
if 'initial_base_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_base_faiss_load_attempted = False
# C·ªù cho Evaluation Document FAISS (M·ªöI)
if 'initial_eval_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_eval_faiss_loaded_toast_shown = False
if 'initial_eval_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_eval_faiss_not_found_toast_shown = False
if 'initial_eval_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_eval_faiss_error_toast_shown = False
if 'initial_eval_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_eval_faiss_load_attempted = False

if "check_button_pressed" not in st.session_state:
    st.session_state.check_button_pressed = False


# --- ƒê·∫∂T ƒê∆Ø·ªúNG D·∫™N C·ª§C B·ªò ---
current_script_directory = os.path.dirname(os.path.abspath(__file__))
local_embedding_model_path = os.path.join(current_script_directory, "local_models", "multilingual-e5-large")
REMOTE_EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
# REMOTE_EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"


# --- H√†m t·∫£i Embedding Model (CH·ªà H√ÄM N√ÄY D√ôNG @st.cache_resource) ---
@st.cache_resource
def get_huggingface_embeddings_model(model_path: str, is_local: bool):
    try:
        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa model c·ª•c b·ªô n·∫øu is_local l√† True
        if is_local and not (os.path.exists(model_path) and
                             (os.path.exists(os.path.join(model_path, 'pytorch_model.bin')) or
                              os.path.exists(os.path.join(model_path, 'model.safetensors')) or
                              os.path.exists(os.path.join(model_path, 'config.json')))):
            return None, "load_error:Local model path provided but no valid model files found."

        embed_model = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'} # C√≥ th·ªÉ ƒë·ªïi th√†nh 'cuda' n·∫øu c√≥ GPU
        )
        return embed_model, "loaded_successfully"
    except Exception as e:
        return None, f"load_error:{e}"

# --- Logic t·∫£i m√¥ h√¨nh Embedding khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng ---
if "embeddings_object" not in st.session_state or st.session_state.embeddings_object is None:
    # 1. Th·ª≠ t·∫£i t·ª´ c·ª•c b·ªô tr∆∞·ªõc
    with st.spinner(f"ƒêang ki·ªÉm tra v√† t·∫£i m√¥ h√¨nh Embedding t·ª´ c·ª•c b·ªô ({local_embedding_model_path})..."):
        embed_model_result, status = get_huggingface_embeddings_model(local_embedding_model_path, is_local=True)

    if embed_model_result:
        st.session_state.embeddings_object = embed_model_result
        if not st.session_state.initial_embed_toast_shown:
            st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ c·ª•c b·ªô!", icon="‚úÖ")
            st.session_state.initial_embed_toast_shown = True
    else:
        # 2. Th·ª≠ t·∫£i t·ª´ Internet n·∫øu c·ª•c b·ªô kh√¥ng th√†nh c√¥ng
        with st.spinner(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh c·ª•c b·ªô. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})..."):
            st.toast(f"M√¥ h√¨nh Embedding c·ª•c b·ªô kh√¥ng t√¨m th·∫•y. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})...", icon="üåê")
            embed_model_result, status = get_huggingface_embeddings_model(REMOTE_EMBEDDING_MODEL_NAME, is_local=False)

        if embed_model_result:
            st.session_state.embeddings_object = embed_model_result
            if not st.session_state.initial_embed_toast_shown:
                st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ Internet!", icon="‚úÖ")
                st.session_state.initial_embed_toast_shown = True
        else:
            # N·∫øu c·∫£ hai c√°ch ƒë·ªÅu l·ªói
            if not st.session_state.initial_embed_error_toast_shown:
                error_message = status.split(":", 1)[1] if ":" in status else status
                st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Embedding: {error_message}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Internet ho·∫∑c ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh.")
                st.session_state.initial_embed_error_toast_shown = True
            st.stop()

embeddings = st.session_state.embeddings_object

BASE_FAISS_PATH = "faiss_index_base_docs" 
EVAL_FAISS_PATH = "faiss_index_eval_docs"

if not os.path.exists(BASE_FAISS_PATH):
    os.makedirs(BASE_FAISS_PATH)
if not os.path.exists(EVAL_FAISS_PATH): # M·ªöI
    os.makedirs(EVAL_FAISS_PATH)

if "base_vector_store" not in st.session_state: 
    st.session_state.base_vector_store = None
if "eval_vector_store" not in st.session_state: 
    st.session_state.eval_vector_store = None


# H√†m n√†y ch·ªâ ch·ª©a logic t·∫£i/ki·ªÉm tra, kh√¥ng c√≥ l·ªánh Streamlit UI
def load_faiss_index_from_disk(path, embeddings_obj):
    if embeddings_obj is None:
        return None, "embeddings_not_ready"

    if os.path.exists(path) and os.listdir(path):
        try:
            vector_store = FAISS.load_local(
                path,
                embeddings_obj, # S·ª≠ d·ª•ng ƒë·ªëi t∆∞·ª£ng embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i v√† cache
                allow_dangerous_deserialization=True
            )
            return vector_store, "loaded_successfully"
        except Exception as e:
            return None, f"load_error:{e}"
    else:
        return None, "not_found"

# --- H√†m x·ª≠ l√Ω file upload ---
def process_uploaded_file(uploaded_file, faiss_path, vector_store_key, doc_type_name):
    if embeddings is None:
        st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√¨ m√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói tr√™n c√πng.")
        return False

    import tempfile
    import shutil
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, uploaded_file.name)

    with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
        try:
            file_extension = uploaded_file.name.split(".")[-1].lower()

            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            docs = []
            if file_extension == "pdf":
                loader = PyPDFLoader(temp_file_path)
                docs = loader.load()
            elif file_extension == "docx":
                loader = UnstructuredWordDocumentLoader(temp_file_path)
                docs = loader.load()
            elif file_extension == "xlsx":
                loader = UnstructuredExcelLoader(temp_file_path)
                docs = loader.load()
            else:
                st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i l√™n file PDF, DOCX, ho·∫∑c XLSX.")
                shutil.rmtree(temp_dir)
                return False

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            splits = text_splitter.split_documents(docs)

            new_vector_store = FAISS.from_documents(splits, embeddings)
            new_vector_store.save_local(faiss_path)
            st.session_state[vector_store_key] = new_vector_store

            st.success(f"T√†i li·ªáu '{doc_type_name}' ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")

            # Reset c√°c c·ªù tr·∫°ng th√°i FAISS c·ªßa lo·∫°i t√†i li·ªáu n√†y
            if vector_store_key == "base_vector_store":
                st.session_state.initial_base_faiss_loaded_toast_shown = False
                st.session_state.initial_base_faiss_not_found_toast_shown = False
                st.session_state.initial_base_faiss_error_toast_shown = False
                st.session_state.initial_base_faiss_load_attempted = False
            elif vector_store_key == "eval_vector_store":
                st.session_state.initial_eval_faiss_loaded_toast_shown = False
                st.session_state.initial_eval_faiss_not_found_toast_shown = False
                st.session_state.initial_eval_faiss_error_toast_shown = False
                st.session_state.initial_eval_faiss_load_attempted = False
            return True

        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}. Vui l√≤ng ki·ªÉm tra file ho·∫∑c c√†i ƒë·∫∑t th∆∞ vi·ªán 'unstructured'.")
            return False
        finally:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)


# --- Logic t·∫£i FAISS ban ƒë·∫ßu cho Base Document ---
if st.session_state.base_vector_store is None and not st.session_state.initial_base_faiss_load_attempted:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu t√†i li·ªáu c∆° s·ªü t·ª´ FAISS index '{BASE_FAISS_PATH}'..."): 
        st.session_state.base_vector_store, status = load_faiss_index_from_disk(BASE_FAISS_PATH, embeddings)

    if status == "loaded_successfully":
        if not st.session_state.initial_base_faiss_loaded_toast_shown: 
            # st.toast(f"FAISS index t√†i li·ªáu c∆° s·ªü ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ '{BASE_FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_base_faiss_loaded_toast_shown = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_base_faiss_error_toast_shown: 
            st.error(f"L·ªói khi t·∫£i FAISS index t√†i li·ªáu c∆° s·ªü: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_base_faiss_error_toast_shown = True
        st.session_state.base_vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_base_faiss_not_found_toast_shown:
            # st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index t√†i li·ªáu c∆° s·ªü trong th∆∞ m·ª•c '{BASE_FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_base_faiss_not_found_toast_shown = True
        st.session_state.base_vector_store = None
    elif status == "embeddings_not_ready":
        pass
    st.session_state.initial_base_faiss_load_attempted = True

# --- Logic t·∫£i FAISS ban ƒë·∫ßu cho Evaluation Document ---
if st.session_state.eval_vector_store is None and not st.session_state.initial_eval_faiss_load_attempted:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu t√†i li·ªáu ƒë√°nh gi√° t·ª´ FAISS index '{EVAL_FAISS_PATH}'..."):
        st.session_state.eval_vector_store, status = load_faiss_index_from_disk(EVAL_FAISS_PATH, embeddings)

    if status == "loaded_successfully":
        if not st.session_state.initial_eval_faiss_loaded_toast_shown:
            # st.toast(f"FAISS index t√†i li·ªáu ƒë√°nh gi√° ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ '{EVAL_FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_eval_faiss_loaded_toast_shown = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_eval_faiss_error_toast_shown:
            st.error(f"L·ªói khi t·∫£i FAISS index t√†i li·ªáu ƒë√°nh gi√°: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_eval_faiss_error_toast_shown = True
        st.session_state.eval_vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_eval_faiss_not_found_toast_shown:
            # st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index t√†i li·ªáu ƒë√°nh gi√° trong th∆∞ m·ª•c '{EVAL_FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_eval_faiss_not_found_toast_shown = True
        st.session_state.eval_vector_store = None
    st.session_state.initial_eval_faiss_load_attempted = True

def handle_canchu_check_button(chat_model, base_store, eval_store):
    if not base_store or not eval_store:
        return "ƒê·ªÉ th·ª±c hi·ªán 'ki·ªÉm tra cƒÉn c·ª©', vui l√≤ng t·∫£i l√™n c·∫£ **T√†i li·ªáu C∆° s·ªü** v√† **T√†i li·ªáu ƒê√°nh gi√°**."

    # Prompt gi·∫£ ƒë·ªãnh cho vi·ªác truy xu·∫•t khi nh·∫•n n√∫t
    # (C√≥ th·ªÉ l√† m·ªôt chu·ªói r·ªóng ho·∫∑c "ki·ªÉm tra" t√πy c√°ch b·∫°n mu·ªën retriever ho·∫°t ƒë·ªông)
    # V√¨ b·∫°n mu·ªën n√≥ ho·∫°t ƒë·ªông gi·ªëng nh∆∞ khi ng∆∞·ªùi d√πng nh·∫≠p "ki·ªÉm tra", ch√∫ng ta s·∫Ω d√πng "ki·ªÉm tra" l√†m prompt ƒë·ªÉ retriever t√¨m ki·∫øm ng·ªØ c·∫£nh.
    button_prompt_query = "ki·ªÉm tra"

    # Truy xu·∫•t ng·ªØ c·∫£nh t·ª´ c·∫£ hai t√†i li·ªáu
    base_docs = []
    eval_docs = []
    base_context = ""
    eval_context = ""

    if base_store:
        base_retriever = base_store.as_retriever(search_kwargs={"k": 3})
        base_docs = base_retriever.invoke(button_prompt_query)
        base_context = "\n".join([doc.page_content for doc in base_docs])

    if eval_store:
        eval_retriever = eval_store.as_retriever(search_kwargs={"k": 3})
        eval_docs = eval_retriever.invoke(button_prompt_query)
        eval_context = "\n".join([doc.page_content for doc in eval_docs])

    # Prompt t√πy ch·ªânh cho t√°c v·ª• "cƒÉn c·ª©"
    final_prompt_template_content = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ tr√≠ch xu·∫•t d·ªØ li·ªáu.
        Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

        1.  **X√°c ƒë·ªãnh c√°c d√≤ng c√≥ t·ª´ "cƒÉn c·ª©"**: T·ª´ "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu ƒê√ÅNH GI√Å", h√£y t√¨m v√† tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c d√≤ng vƒÉn b·∫£n b·∫Øt ƒë·∫ßu b·∫±ng t·ª´ "cƒÉn c·ª©".
        2.  **T√¨m ki·∫øm trong T√†i li·ªáu C∆° s·ªü**: V·ªõi m·ªói d√≤ng "cƒÉn c·ª©" ƒë√£ tr√≠ch xu·∫•t t·ª´ T√†i li·ªáu ƒê√°nh gi√°, h√£y s·ª≠ d·ª•ng n·ªôi dung c·ªßa d√≤ng ƒë√≥ l√†m truy v·∫•n ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin li√™n quan v√† kh·ªõp trong "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû". H√£y t√¨m c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ n·ªôi dung t∆∞∆°ng ƒë·ªìng ho·∫∑c li√™n quan ch·∫∑t ch·∫Ω (kho·∫£ng 90% ƒë·ªô kh·ªõp v·ªÅ √Ω nghƒ©a) v·ªõi d√≤ng "cƒÉn c·ª©" ƒë√≥, kh√¥ng c·∫ßn ph·∫£i kh·ªõp ch√≠nh x√°c t·ª´ng t·ª´.
        3.  **ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ d·∫°ng b·∫£ng**: Tr·∫£ v·ªÅ th√¥ng tin t√¨m ƒë∆∞·ª£c t·ª´ "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû" d∆∞·ªõi d·∫°ng m·ªôt b·∫£ng duy nh·∫•t.
            * **Quan tr·ªçng:** B·∫£ng n√†y ph·∫£i ch·ª©a **t·∫•t c·∫£ c√°c c·ªôt d·ªØ li·ªáu c√≥ th·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c** t·ª´ T√†i li·ªáu C∆° s·ªü li√™n quan ƒë·∫øn th√¥ng tin t√¨m ƒë∆∞·ª£c. N·∫øu T√†i li·ªáu C∆° s·ªü c·ªßa b·∫°n c√≥ c·∫•u tr√∫c gi·ªëng b·∫£ng ho·∫∑c d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c ph√¢n lo·∫°i th√†nh c√°c c·ªôt (v√≠ d·ª•: "Ti√™u ƒë·ªÅ", "N·ªôi dung", "Ng√†y", "M√£", "M√¥ t·∫£", v.v.), h√£y s·ª≠ d·ª•ng ch√∫ng l√†m t√™n c·ªôt. N·∫øu kh√¥ng, h√£y s·ª≠ d·ª•ng c√°c ti√™u ƒë·ªÅ c·ªôt h·ª£p l√Ω nh·∫•t ƒë·ªÉ tr√¨nh b√†y th√¥ng tin m·ªôt c√°ch c√≥ c·∫•u tr√∫c.
            * M·ªói h√†ng trong b·∫£ng ph·∫£i l√† m·ªôt m·ª•c d·ªØ li·ªáu kh·ªõp ƒë∆∞·ª£c t√¨m th·∫•y trong T√†i li·ªáu C∆° s·ªü.

        C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {button_prompt_query}

        ---
        Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû:
        {base_context}
        ---
        Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu ƒê√ÅNH GI√Å:
        {eval_context}
        ---

        N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan ƒë·ªÉ t·∫°o b·∫£ng ho·∫∑c kh√¥ng th·ªÉ ƒë·ªãnh d·∫°ng th√†nh b·∫£ng, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p trong t√†i li·ªáu, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi. Tr·∫£ l·ªùi tr·ª±c ti·∫øp b·∫±ng b·∫£ng (ho·∫∑c th√¥ng b√°o kh√¥ng t√¨m th·∫•y) m√† kh√¥ng c√≥ b·∫•t k·ª≥ l·ªùi d·∫´n hay k·∫øt lu·∫≠n n√†o.
        """

    messages_for_llm = ChatPromptTemplate.from_messages([
        ("system", final_prompt_template_content),
        ("human", button_prompt_query)
    ])

    chain = messages_for_llm | chat_model
    try:
        response = chain.invoke({"input": button_prompt_query})
        return response.content
    except Exception as e:
        return f"C√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}"

# --- Menu ƒëi·ªÅu h∆∞·ªõng d·ªçc ·ªü Sidebar ---
with st.sidebar:
    selected = option_menu(
        menu_title=None,
        options=["Chatbot", "Qu·∫£n l√Ω d·ªØ li·ªáu"],
        icons=["chat", "file-earmark-text"],
        menu_icon="list",
        default_index=0,
        orientation="vertical",
        styles={
            "container": {"padding": "0!important", "background-color": "transparent"},
            "icon": {"color": "orange", "font-size": "16px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#2ca8f2"},
        }
    )

# --- Logic hi·ªÉn th·ªã n·ªôi dung d·ª±a tr√™n l·ª±a ch·ªçn menu ---
if selected == "Chatbot":
    st.title("ü§ñ Chatbot")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                answer = ""
                retrieved_docs = []
                base_context = ""
                eval_context = ""

                if st.session_state.base_vector_store:
                    base_retriever = st.session_state.base_vector_store.as_retriever(search_kwargs={"k": 3})
                    base_docs = base_retriever.invoke(prompt)
                    retrieved_docs.extend(base_docs)
                    base_context = "\n".join([doc.page_content for doc in base_docs])

                if st.session_state.eval_vector_store:
                    eval_retriever = st.session_state.eval_vector_store.as_retriever(search_kwargs={"k": 3})
                    eval_docs = eval_retriever.invoke(prompt)
                    retrieved_docs.extend(eval_docs)
                    eval_context = "\n".join([doc.page_content for doc in eval_docs])
                

                if prompt.lower() == "ki·ªÉm tra":
                    if st.session_state.base_vector_store and st.session_state.eval_vector_store:
                        final_prompt_template_content = f"""
                            B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ tr√≠ch xu·∫•t d·ªØ li·ªáu.
                            Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

                            1.  **X√°c ƒë·ªãnh c√°c d√≤ng c√≥ t·ª´ "cƒÉn c·ª©"**: T·ª´ "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu ƒê√ÅNH GI√Å", h√£y t√¨m v√† tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c d√≤ng vƒÉn b·∫£n b·∫Øt ƒë·∫ßu b·∫±ng t·ª´ "cƒÉn c·ª©".
                            2.  **T√¨m ki·∫øm trong T√†i li·ªáu C∆° s·ªü**: V·ªõi m·ªói d√≤ng "cƒÉn c·ª©" ƒë√£ tr√≠ch xu·∫•t t·ª´ T√†i li·ªáu ƒê√°nh gi√°, h√£y s·ª≠ d·ª•ng n·ªôi dung c·ªßa d√≤ng ƒë√≥ l√†m truy v·∫•n ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin li√™n quan v√† kh·ªõp trong "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû". H√£y t√¨m c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ n·ªôi dung t∆∞∆°ng ƒë·ªìng ho·∫∑c li√™n quan ch·∫∑t ch·∫Ω (kho·∫£ng 90% ƒë·ªô kh·ªõp v·ªÅ √Ω nghƒ©a) v·ªõi d√≤ng "cƒÉn c·ª©" ƒë√≥, kh√¥ng c·∫ßn ph·∫£i kh·ªõp ch√≠nh x√°c t·ª´ng t·ª´.
                            3.  **ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ d·∫°ng b·∫£ng**: Tr·∫£ v·ªÅ th√¥ng tin t√¨m ƒë∆∞·ª£c t·ª´ "Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû" d∆∞·ªõi d·∫°ng m·ªôt b·∫£ng duy nh·∫•t.
                                * **Quan tr·ªçng:** B·∫£ng n√†y ph·∫£i ch·ª©a **t·∫•t c·∫£ c√°c c·ªôt d·ªØ li·ªáu c√≥ th·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c** t·ª´ T√†i li·ªáu C∆° s·ªü li√™n quan ƒë·∫øn th√¥ng tin t√¨m ƒë∆∞·ª£c. N·∫øu T√†i li·ªáu C∆° s·ªü c·ªßa b·∫°n c√≥ c·∫•u tr√∫c gi·ªëng b·∫£ng ho·∫∑c d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c ph√¢n lo·∫°i th√†nh c√°c c·ªôt (v√≠ d·ª•: "Ti√™u ƒë·ªÅ", "N·ªôi dung", "Ng√†y", "M√£", "M√¥ t·∫£", v.v.), h√£y s·ª≠ d·ª•ng ch√∫ng l√†m t√™n c·ªôt. N·∫øu kh√¥ng, h√£y s·ª≠ d·ª•ng c√°c ti√™u ƒë·ªÅ c·ªôt h·ª£p l√Ω nh·∫•t ƒë·ªÉ tr√¨nh b√†y th√¥ng tin m·ªôt c√°ch c√≥ c·∫•u tr√∫c.
                                * M·ªói h√†ng trong b·∫£ng ph·∫£i l√† m·ªôt m·ª•c d·ªØ li·ªáu kh·ªõp ƒë∆∞·ª£c t√¨m th·∫•y trong T√†i li·ªáu C∆° s·ªü.

                            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {prompt}

                            ---
                            Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu C∆† S·ªû:
                            {base_context}
                            ---
                            Ng·ªØ c·∫£nh t·ª´ T√†i li·ªáu ƒê√ÅNH GI√Å:
                            {eval_context}
                            ---

                            N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan ƒë·ªÉ t·∫°o b·∫£ng ho·∫∑c kh√¥ng th·ªÉ ƒë·ªãnh d·∫°ng th√†nh b·∫£ng, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p trong t√†i li·ªáu, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi. Tr·∫£ l·ªùi tr·ª±c ti·∫øp b·∫±ng b·∫£ng (ho·∫∑c th√¥ng b√°o kh√¥ng t√¨m th·∫•y) m√† kh√¥ng c√≥ b·∫•t k·ª≥ l·ªùi d·∫´n hay k·∫øt lu·∫≠n n√†o.
                            """

                        messages_for_llm = ChatPromptTemplate.from_messages([
                            ("system", final_prompt_template_content),
                            ("human", prompt)
                        ])

                        chain = messages_for_llm | chat
                        response = chain.invoke({"input": prompt})
                        answer = response.content

                    else:
                        answer = "ƒê·ªÉ th·ª±c hi·ªán 'ki·ªÉm tra' theo y√™u c·∫ßu, vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ t·∫£i l√™n c·∫£ **T√†i li·ªáu C∆° s·ªü** v√† **T√†i li·ªáu ƒê√°nh gi√°** trong m·ª•c 'Qu·∫£n l√Ω d·ªØ li·ªáu'."
                        st.warning(answer)
                    st.session_state.check_button_pressed = False

                else:
                    if st.session_state.base_vector_store or st.session_state.eval_vector_store:
                        if not retrieved_docs:
                            answer = "Kh√¥ng t√¨m ƒë∆∞·ª£c k·∫øt qu·∫£ ph√π h·ª£p trong t√†i li·ªáu."
                        else:
                            rag_chain = create_stuff_documents_chain(chat, rag_prompt)
                            response = rag_chain.invoke({"context": retrieved_docs, "input": prompt})
                            if isinstance(response, dict) and "answer" in response:
                                answer = response["answer"]
                            else:
                                answer = response

                            if not answer or answer.strip().lower() == "t√¥i kh√¥ng bi·∫øt." or "kh√¥ng t√¨m th·∫•y th√¥ng tin" in answer.lower():
                                answer = "Kh√¥ng t√¨m ƒë∆∞·ª£c k·∫øt qu·∫£ ph√π h·ª£p trong t√†i li·ªáu."
                    else:
                        st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu t√†i li·ªáu n√†o. Chatbot s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                        default_prompt = ChatPromptTemplate.from_messages([
                            ("system", PROMPT_TEMPLATE),
                            ("human", "{input}")
                        ])
                        chain = default_prompt | chat
                        response = chain.invoke({"input": prompt})
                        answer = response.content

                with st.chat_message("assistant"):
                    st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

elif selected == "Qu·∫£n l√Ω d·ªØ li·ªáu":
    st.header("T·∫£i l√™n T√†i li·ªáu")

    if embeddings is None:
        st.warning("M√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt.")

    st.info("T·∫£i l√™n file PDF, DOCX ho·∫∑c XLSX. Sau ƒë√≥, ch·ªçn lo·∫°i t√†i li·ªáu (C∆° s·ªü ho·∫∑c ƒê√°nh gi√°).")
    # Th√™m Radio Button ƒë·ªÉ ch·ªçn lo·∫°i t√†i li·ªáu (M·ªöI)
    document_type = st.radio(
        "ƒê√¢y l√† lo·∫°i t√†i li·ªáu g√¨?",
        ("T√†i li·ªáu C∆° s·ªü", "T√†i li·ªáu c·∫ßn Ki·ªÉm tra"),
        key="doc_type_selector",
        index=1, # M·∫∑c ƒë·ªãnh l√† "T√†i li·ªáu ƒê√°nh gi√°/Ki·ªÉm tra"
        help="Ch·ªçn 'T√†i li·ªáu C∆° s·ªü' cho d·ªØ li·ªáu ch√≠nh c·ªßa chatbot, ho·∫∑c 'T√†i li·ªáu c·∫ßn Ki·ªÉm tra' cho d·ªØ li·ªáu ph·ª• ƒë·ªÉ so s√°nh."
    )

    uploaded_file = st.file_uploader(
        "Ch·ªçn m·ªôt file ƒë·ªÉ t·∫£i l√™n",
        type=["pdf", "txt", "docx", "xlsx"],
        key="single_file_uploader",
        help="T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n. B·∫°n s·∫Ω ch·ªçn ƒë√¢y l√† t√†i li·ªáu c∆° s·ªü hay t√†i li·ªáu ƒë√°nh gi√°."
    )

    if uploaded_file:
        if document_type == "T√†i li·ªáu C∆° s·ªü":
            target_faiss_path = BASE_FAISS_PATH
            target_vector_store_key = "base_vector_store"
            doc_type_name = "T√†i li·ªáu C∆° s·ªü"
        else:
            target_faiss_path = EVAL_FAISS_PATH
            target_vector_store_key = "eval_vector_store"
            doc_type_name = "T√†i li·ªáu c·∫ßn Ki·ªÉm tra"

        current_file_id = f"{uploaded_file.name}-{uploaded_file.size}-{document_type}"

        # Ki·ªÉm tra xem file hi·ªán t·∫°i c√≥ kh√°c v·ªõi file ƒë√£ x·ª≠ l√Ω g·∫ßn nh·∫•t cho lo·∫°i t√†i li·ªáu n√†y kh√¥ng
        if st.session_state.get(f'last_processed_file_{target_vector_store_key}') != current_file_id:
            if process_uploaded_file(uploaded_file, target_faiss_path, target_vector_store_key, doc_type_name):
                st.session_state[f'last_processed_file_{target_vector_store_key}'] = current_file_id
                st.rerun()
        else:
            pass

    st.markdown("---")
    st.subheader("Tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán t·∫°i:")
    col1, col2 = st.columns(2)
    with col1:
        st.write("#### T√†i li·ªáu C∆° s·ªü:")
        if st.session_state.base_vector_store:
            st.write("‚úÖ ƒê√£ t·∫£i v√† s·∫µn s√†ng.")
        else:
            st.write("‚ùå Ch∆∞a t·∫£i.")
        if st.button("X√≥a T√†i li·ªáu C∆° s·ªü", key="delete_base_faiss"):
            if os.path.exists(BASE_FAISS_PATH):
                import shutil
                try:
                    shutil.rmtree(BASE_FAISS_PATH)
                    st.session_state.base_vector_store = None
                    st.session_state.initial_base_faiss_loaded_toast_shown = False
                    st.session_state.initial_base_faiss_not_found_toast_shown = False
                    st.session_state.initial_base_faiss_error_toast_shown = False
                    st.session_state.initial_base_faiss_load_attempted = False
                    st.success("T√†i li·ªáu C∆° s·ªü ƒë√£ ƒë∆∞·ª£c x√≥a.")
                    st.rerun()
                except Exception as e:
                    st.error(f"L·ªói khi x√≥a T√†i li·ªáu C∆° s·ªü: {e}")
            else:
                st.info("Kh√¥ng t√¨m th·∫•y T√†i li·ªáu C∆° s·ªü ƒë·ªÉ x√≥a.")

    with col2:
        st.write("#### T√†i li·ªáu c·∫ßn Ki·ªÉm tra:")
        if st.session_state.eval_vector_store:
            st.write("‚úÖ ƒê√£ t·∫£i v√† s·∫µn s√†ng.")
        else:
            st.write("‚ùå Ch∆∞a t·∫£i.")
        if st.button("X√≥a T√†i li·ªáu c·∫ßn ki·ªÉm tra", key="delete_eval_faiss"):
            if os.path.exists(EVAL_FAISS_PATH):
                import shutil
                try:
                    shutil.rmtree(EVAL_FAISS_PATH)
                    st.session_state.eval_vector_store = None
                    st.session_state.initial_eval_faiss_loaded_toast_shown = False
                    st.session_state.initial_eval_faiss_not_found_toast_shown = False
                    st.session_state.initial_eval_faiss_error_toast_shown = False
                    st.session_state.initial_eval_faiss_load_attempted = False
                    st.success("FAISS index T√†i li·ªáu c·∫ßn ki·ªÉm tra ƒë√£ ƒë∆∞·ª£c x√≥a.")
                    st.rerun()
                except Exception as e:
                    st.error(f"L·ªói khi x√≥a T√†i li·ªáu c·∫ßn ki·ªÉm tra: {e}")
            else:
                st.info("Kh√¥ng t√¨m th·∫•y T√†i li·ªáu c·∫ßn ki·ªÉm tra ƒë·ªÉ x√≥a.")


    st.subheader("Ki·ªÉm tra v√† Tr√≠ch xu·∫•t D·ªØ li·ªáu")
    st.write("Nh·∫•n n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra t·ª± ƒë·ªông")

    
    if st.button("Th·ª±c hi·ªán Ki·ªÉm tra"):

        canchu_check_result_placeholder = st.empty()
        with st.spinner("ƒêang th·ª±c hi·ªán ki·ªÉm tra"):
            result_for_button = handle_canchu_check_button(
                chat,
                st.session_state.get("base_vector_store"),
                st.session_state.get("eval_vector_store")
            )
    canchu_check_result_placeholder.markdown(result_for_button)