import os
import streamlit as st
# import google.generativeai as genai
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from streamlit_option_menu import option_menu
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
import pandas as pd
import re
from langchain_groq import ChatGroq

from pypdf import PdfReader
from pdf2image import convert_from_path
from paddleocr import PaddleOCR
import numpy as np
from PIL import Image
import unicodedata

load_dotenv()

st.set_page_config(page_title="ü§ñ Chatbot AI", layout="wide")

# genai_api_key = os.getenv('GOOGLE_API_KEY')
# if not genai_api_key:
#     st.error("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Gemini.")
#     st.stop()

# genai.configure(api_key=genai_api_key)

# chat = ChatGoogleGenerativeAI(
#     model="gemini-1.5-flash",
#     temperature=0,
#     google_api_key=genai_api_key,
#     convert_system_message_to_human=True
# )

groq_api_key = os.getenv('GROQ_API_KEY')
if not groq_api_key:
    st.error("GROQ_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Groq.")
    st.stop()

chat = ChatGroq(
    temperature=0,
    groq_api_key=groq_api_key,
    model_name="llama3-8b-8192"
)

PROMPT_TEMPLATE = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.
    S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.

    Ng·ªØ c·∫£nh:
    {context}
    """
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", PROMPT_TEMPLATE),
    ("human", "{input}")
])


# --- Kh·ªüi t·∫°o Session State Flags cho th√¥ng b√°o ---
if 'initial_embed_toast_shown' not in st.session_state:
    st.session_state.initial_embed_toast_shown = False
if 'initial_embed_error_toast_shown' not in st.session_state:
    st.session_state.initial_embed_error_toast_shown = False
if 'initial_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_loaded_toast_shown = False
if 'initial_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_not_found_toast_shown = False
if 'initial_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_error_toast_shown = False
if 'initial_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_faiss_load_attempted = False


# --- ƒê·∫∂T ƒê∆Ø·ªúNG D·∫™N C·ª§C B·ªò ---
current_script_directory = os.path.dirname(os.path.abspath(__file__))
local_embedding_model_path = os.path.join(current_script_directory, "local_models", "multilingual-e5-large")
REMOTE_EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
# REMOTE_EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"


# --- H√†m t·∫£i Embedding Model (CH·ªà H√ÄM N√ÄY D√ôNG @st.cache_resource) ---
@st.cache_resource
def get_huggingface_embeddings_model(model_path: str, is_local: bool):
    try:
        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa model c·ª•c b·ªô n·∫øu is_local l√† True
        if is_local and not (os.path.exists(model_path) and
                             (os.path.exists(os.path.join(model_path, 'pytorch_model.bin')) or
                              os.path.exists(os.path.join(model_path, 'model.safetensors')) or
                              os.path.exists(os.path.join(model_path, 'config.json')))):
            return None, "load_error:Local model path provided but no valid model files found."

        embed_model = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'}
        )
        return embed_model, "loaded_successfully"
    except Exception as e:
        return None, f"load_error:{e}"

# --- Logic t·∫£i m√¥ h√¨nh Embedding khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng ---
if "embeddings_object" not in st.session_state or st.session_state.embeddings_object is None:
    # 1. Th·ª≠ t·∫£i t·ª´ c·ª•c b·ªô tr∆∞·ªõc
    with st.spinner(f"ƒêang ki·ªÉm tra v√† t·∫£i m√¥ h√¨nh Embedding t·ª´ c·ª•c b·ªô ({local_embedding_model_path})..."):
        embed_model_result, status = get_huggingface_embeddings_model(local_embedding_model_path, is_local=True)

    if embed_model_result:
        st.session_state.embeddings_object = embed_model_result
        if not st.session_state.initial_embed_toast_shown:
            st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ c·ª•c b·ªô!", icon="‚úÖ")
            st.session_state.initial_embed_toast_shown = True
    else:
        # 2. Th·ª≠ t·∫£i t·ª´ Internet n·∫øu c·ª•c b·ªô kh√¥ng th√†nh c√¥ng
        with st.spinner(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh c·ª•c b·ªô. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})..."):
            st.toast(f"M√¥ h√¨nh Embedding c·ª•c b·ªô kh√¥ng t√¨m th·∫•y. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})...", icon="üåê")
            embed_model_result, status = get_huggingface_embeddings_model(REMOTE_EMBEDDING_MODEL_NAME, is_local=False)

        if embed_model_result:
            st.session_state.embeddings_object = embed_model_result
            if not st.session_state.initial_embed_toast_shown:
                st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ Internet!", icon="‚úÖ")
                st.session_state.initial_embed_toast_shown = True
        else:
            if not st.session_state.initial_embed_error_toast_shown:
                error_message = status.split(":", 1)[1] if ":" in status else status
                st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Embedding: {error_message}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Internet ho·∫∑c ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh.")
                st.session_state.initial_embed_error_toast_shown = True
            st.stop()

embeddings = st.session_state.embeddings_object

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u FAISS index ---
FAISS_PATH = "faiss_index_data_multilingual"
if not os.path.exists(FAISS_PATH):
    os.makedirs(FAISS_PATH)

# --- Qu·∫£n l√Ω FAISS Vector Store trong session state ---
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "processing_done" not in st.session_state:
    st.session_state.processing_done = False


# --- H√†m t·∫£i FAISS index (KH√îNG D√ôNG @st.cache_resource) ---
# H√†m n√†y ch·ªâ ch·ª©a logic t·∫£i/ki·ªÉm tra, kh√¥ng c√≥ l·ªánh Streamlit UI
def load_faiss_index_from_disk(path, embeddings_obj):
    if embeddings_obj is None: # ƒê·∫£m b·∫£o embeddings ƒë√£ s·∫µn s√†ng
        return None, "embeddings_not_ready"

    if os.path.exists(path) and os.listdir(path):
        try:
            vector_store = FAISS.load_local(
                path,
                embeddings_obj, # S·ª≠ d·ª•ng ƒë·ªëi t∆∞·ª£ng embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i v√† cache
                allow_dangerous_deserialization=True
            )
            return vector_store, "loaded_successfully"
        except Exception as e:
            return None, f"load_error:{e}"
    else:
        return None, "not_found"



# Ch·ªâ c·ªë g·∫Øng t·∫£i FAISS t·ª´ ƒëƒ©a n·∫øu n√≥ ch∆∞a ƒë∆∞·ª£c t·∫£i v√†o session_state V√Ä ch∆∞a t·ª´ng th·ª≠ t·∫£i
if st.session_state.vector_store is None and not st.session_state.initial_faiss_load_attempted:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu AI t·ª´ FAISS index ƒë√£ l∆∞u trong '{FAISS_PATH}'..."):
        st.session_state.vector_store, status = load_faiss_index_from_disk(FAISS_PATH, embeddings) # Truy·ªÅn embeddings ƒë√£ ƒë∆∞·ª£c cache

    if status == "loaded_successfully":
        if not st.session_state.initial_faiss_loaded_toast_shown:
            st.toast(f"FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ th∆∞ m·ª•c '{FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_faiss_loaded_toast_shown = True
        st.session_state.processing_done = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_faiss_error_toast_shown:
            st.error(f"L·ªói khi t·∫£i FAISS index t·ª´ c·ª•c b·ªô: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_faiss_error_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_faiss_not_found_toast_shown:
            st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index trong th∆∞ m·ª•c '{FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_faiss_not_found_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "embeddings_not_ready":
        # ƒêi·ªÅu n√†y s·∫Ω kh√¥ng x·∫£y ra n·∫øu logic t·∫£i embeddings ch·∫°y tr∆∞·ªõc
        pass
    st.session_state.initial_faiss_load_attempted = True # ƒê√°nh d·∫•u l√† ƒë√£ th·ª≠ t·∫£i l·∫ßn ƒë·∫ßu

def read_excel_to_array():
    file_path = "./data/1. AI . DM n·ªôi dung tr√¨nh HƒêTV t·ª´ Ban K·∫ø ho·∫°ch (B02) final.xlsx"
    sheet_name = "Danh s√°ch vƒÉn b·∫£n"
    try:
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
        else:
            df = pd.read_excel(file_path)
        data_array = df.values.tolist()
        return data_array
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n '{file_path}'")
        return None
    except KeyError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y sheet '{sheet_name}' trong file.")
        return None
    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói khi ƒë·ªçc file Excel: {e}")
        return None
    
def handleCheck(llm_model, vector_store):
    data_from_excel = read_excel_to_array()
    if vector_store:
        try:
            retriever = vector_store.as_retriever()
            check_prompt_template = """
                B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v√† t·ªâ m·ªâ.

                **Nhi·ªám v·ª•:**
                T·ª´ ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi, h√£y **t√¨m v√† li·ªát k√™ ch√≠nh x√°c T·∫§T C·∫¢ c√°c d√≤ng ho·∫∑c ƒëo·∫°n vƒÉn b·∫£n** m√† trong ƒë√≥ xu·∫•t hi·ªán t·ª´ ho·∫∑c c·ª•m t·ª´ **'cƒÉn c·ª©'**.

                **L∆∞u √Ω quan tr·ªçng:**
                * H√£y ch√∫ √Ω ƒë·∫øn **ng·ªØ c·∫£nh v√† c·∫•u tr√∫c c√¢u** ƒë·ªÉ ƒë·∫£m b·∫£o 'cƒÉn c·ª©' ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë√∫ng nghƒ©a.
                * **Tr√°nh c√°c l·ªói ƒë·ªãnh d·∫°ng ho·∫∑c k√Ω t·ª± l·∫°** khi tr√≠ch xu·∫•t. Ch·ªâ tr·∫£ v·ªÅ ph·∫ßn vƒÉn b·∫£n g·ªëc, chu·∫©n x√°c.
                * N·∫øu m·ªôt d√≤ng/ƒëo·∫°n c√≥ ch·ª©a 'cƒÉn c·ª©' v√† sau ƒë√≥ l·∫°i b·ªã s·ª≠a ƒë·ªïi ho·∫∑c h·ªßy b·ªè b·ªüi m·ªôt ghi ch√∫, b·∫°n v·∫´n li·ªát k√™ n√≥ nh∆∞ng c√≥ th·ªÉ ghi ch√∫ th√™m n·∫øu th√¥ng tin ƒë√≥ r√µ r√†ng trong ng·ªØ c·∫£nh.

                **ƒê·ªãnh d·∫°ng k·∫øt qu·∫£:**
                Li·ªát k√™ m·ªói d√≤ng/ƒëo·∫°n t√¨m ƒë∆∞·ª£c tr√™n m·ªôt d√≤ng ri√™ng.
                N·∫øu kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ d√≤ng/ƒëo·∫°n n√†o ch·ª©a 'cƒÉn c·ª©', h√£y tr·∫£ l·ªùi r√µ r√†ng: "Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ d·ªØ li·ªáu trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p."

                **Ng·ªØ c·∫£nh:**
                {context}
                """
            check_rag_prompt = ChatPromptTemplate.from_messages([
                ("system", check_prompt_template),
                ("human", "{input}")
            ])

            combine_docs_chain = create_stuff_documents_chain(llm_model, check_rag_prompt)
            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

            response = retrieval_chain.invoke({"input": "T√¨m nh·ªØng d√≤ng c√≥ t·ª´ 'cƒÉn c·ª©' trong vƒÉn b·∫£n"})

            results = []
            date_pattern = re.compile(r'(\d{1,2})/(\d{1,2})/(\d{4})')
            for item in data_from_excel:
                stt = item[0] 
                doc_title = item[1] 
                found_date_str_in_title = None 
                status = item[3]

                if isinstance(doc_title, str):
                    match_obj = date_pattern.search(doc_title)
                    
                    if match_obj:
                        found_date_str_in_title = match_obj.group(0)
                text_matches = isinstance(doc_title, str) and doc_title.lower() in response["answer"].lower()
                date_matches = False
                if found_date_str_in_title:
                    date_matches = found_date_str_in_title in response["answer"] 

                if text_matches or date_matches:
                    new_record = [stt, doc_title, found_date_str_in_title if found_date_str_in_title is not None else "", status if status is not None else ""]
                    results.append(new_record)
            print("results", results)
            print("response[]", response["answer"])
            return results
        except Exception as e:
            return f"L·ªói khi th·ª±c hi·ªán ki·ªÉm tra: {str(e)}"
    else:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu t·∫£i l√™n ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu tr∆∞·ªõc."



# --- H√†m ki·ªÉm tra xem PDF c√≥ text layer hay kh√¥ng ---
def has_text_layer(pdf_path: str) -> bool:
    try:
        reader = PdfReader(pdf_path)
        for page in reader.pages:
            # Ki·ªÉm tra xem trang c√≥ b·∫•t k·ª≥ vƒÉn b·∫£n n√†o kh√¥ng
            if page.extract_text():
                return True
        return False
    except Exception as e:
        st.warning(f"Kh√¥ng th·ªÉ ki·ªÉm tra text layer c·ªßa PDF: {e}. Coi nh∆∞ kh√¥ng c√≥ text layer v√† s·∫Ω d√πng OCR.")
        return False # Gi·∫£ ƒë·ªãnh kh√¥ng c√≥ ƒë·ªÉ d√πng OCR

# --- H√†m OCR PDF b·∫±ng PaddleOCR ---
@st.cache_resource
def get_paddleocr_model():
    # T·∫£i m√¥ h√¨nh PaddleOCR m·ªôt l·∫ßn v√† cache l·∫°i
    # lang='vi' cho ti·∫øng Vi·ªát, use_gpu=False n·∫øu kh√¥ng c√≥ GPU ho·∫∑c kh√¥ng mu·ªën d√πng
    # show_log=False ƒë·ªÉ ·∫©n log t·∫£i m√¥ h√¨nh n·∫øu kh√¥ng c·∫ßn thi·∫øt
    return PaddleOCR(lang='vi', use_gpu=False, det_model_dir=None, rec_model_dir=None, cls_model_dir=None, show_log=False)


# --- H√†m OCR PDF b·∫±ng PaddleOCR ---
@st.cache_resource
def get_paddleocr_model():
    # T·∫£i m√¥ h√¨nh PaddleOCR m·ªôt l·∫ßn v√† cache l·∫°i
    # lang='vi' cho ti·∫øng Vi·ªát, use_gpu=False n·∫øu kh√¥ng c√≥ GPU ho·∫∑c kh√¥ng mu·ªën d√πng
    # show_log=False ƒë·ªÉ ·∫©n log t·∫£i m√¥ h√¨nh n·∫øu kh√¥ng c·∫ßn thi·∫øt
    return PaddleOCR(lang='vi', use_gpu=False, det_model_dir=None, rec_model_dir=None, cls_model_dir=None, show_log=False)

def ocr_pdf(pdf_path: str, ocr_model) -> str:
    text_content = []
    images = convert_from_path(pdf_path) # Chuy·ªÉn m·ªói trang PDF th√†nh ·∫£nh

    for i, image in enumerate(images):
        try:
            # Chuy·ªÉn ·∫£nh PIL sang d·∫°ng numpy array ƒë·ªÉ PaddleOCR x·ª≠ l√Ω
            img_np = np.array(image)
            result = ocr_model.ocr(img_np, cls=True) # cls=True ƒë·ªÉ nh·∫≠n d·∫°ng ch·ªØ d·ªçc

            if result and result[0]: # result[0] ch·ª©a c√°c k·∫øt qu·∫£ t·ª´ng d√≤ng
                for line in result[0]:
                    if line[1][0]: # line[1][0] l√† vƒÉn b·∫£n tr√≠ch xu·∫•t
                        text_content.append(line[1][0])
            else:
                st.warning(f"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ trang {i+1} c·ªßa PDF b·∫±ng OCR.")
        except Exception as e:
            st.error(f"L·ªói OCR trang {i+1}: {e}")
            continue
    return "\n".join(text_content)

# --- C√°c h√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ---
def normalize_text(text):
    if not isinstance(text, str):
        return text
    text = unicodedata.normalize('NFC', text)
    text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C')
    return text.strip()

def fix_common_ocr_errors(text):
    if not isinstance(text, str):
        return text
    replacements = {
        'c√º': 'cƒÉn c·ª©', 'k&': 'k·∫ø', 'drng': 'd·ª±ng', 'n√§rn': 'nƒÉm',
        'Di&n': 'ƒêi·ªán', 'thijc hin': 'th·ª±c hi·ªán', 'cp nht': 'c·∫≠p nh·∫≠t',
        'san xut': 's·∫£n xu·∫•t', 'diu in': 'ƒëi·ªÅu h√†nh', # Ho·∫∑c 'ƒëi·ªán' t√πy ng·ªØ c·∫£nh ch√≠nh x√°c
        'ph√°t tri&i': 'ph√°t tri·ªÉn', 'giai doan': 'giai ƒëo·∫°n',
        'TP 1J√† NOi': 'TP H√† N·ªôi', '1J': 'H', # R·∫•t ph·ªï bi·∫øn l·ªói n√†y trong OCR ti·∫øng Vi·ªát
        'V√§n ban': 'VƒÉn b·∫£n', 'Ban k& hoch': 'Ban k·∫ø ho·∫°ch',
        # Th√™m c√°c c·∫∑p l·ªói-s·ª≠a kh√°c m√† b·∫°n quan s√°t ƒë∆∞·ª£c
    }
    for wrong, correct in replacements.items():
        text = re.sub(re.escape(wrong), correct, text, flags=re.IGNORECASE)
    return text

def remove_non_alphanumeric_and_normalize_space(text):
    if not isinstance(text, str):
        return text
    # Gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë, k√Ω t·ª± ti·∫øng Vi·ªát, d·∫•u c√¢u c∆° b·∫£n v√† kho·∫£ng tr·∫Øng
    text = re.sub(r'[^\w\s.,!?;√Ä√Å·∫†·∫¢√ÉƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√Ç·∫¶·∫§·∫¨·∫®·∫™√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒêƒë]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# --- H√†m x·ª≠ l√Ω t√†i li·ªáu t·ªïng qu√°t ---
def process_document(file_path: str, file_extension: str, embeddings_obj):
    docs = []
    if file_extension == "pdf":
        if has_text_layer(file_path):
            st.info("Ph√°t hi·ªán PDF c√≥ l·ªõp vƒÉn b·∫£n. ƒêang tr√≠ch xu·∫•t vƒÉn b·∫£n tr·ª±c ti·∫øp.")
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        else:
            st.warning("Ph√°t hi·ªán PDF d·∫°ng ·∫£nh scan ho·∫∑c kh√¥ng c√≥ l·ªõp vƒÉn b·∫£n. ƒêang ti·∫øn h√†nh OCR...")
            with st.spinner("ƒêang t·∫£i m√¥ h√¨nh OCR (ch·ªâ l·∫ßn ƒë·∫ßu) v√† x·ª≠ l√Ω OCR..."):
                ocr_model = get_paddleocr_model()
                ocr_text = ocr_pdf(file_path, ocr_model)
            if ocr_text:
                # PaddleOCR tr·∫£ v·ªÅ m·ªôt chu·ªói vƒÉn b·∫£n, t·∫°o m·ªôt Document t·ª´ chu·ªói ƒë√≥
                from langchain_core.documents import Document
                docs.append(Document(page_content=ocr_text, metadata={"source": file_path, "ocr": True}))
            else:
                st.error("OCR kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ PDF.")
                return None
    elif file_extension == "docx":
        loader = UnstructuredWordDocumentLoader(file_path)
        docs = loader.load()
    elif file_extension == "xlsx":
        loader = UnstructuredExcelLoader(file_path)
        docs = loader.load()
    else:
        st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i l√™n file PDF, DOCX, ho·∫∑c XLSX.")
        return None

    # √Åp d·ª•ng c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n cho t·∫•t c·∫£ c√°c t√†i li·ªáu
    processed_docs = []
    for doc in docs:
        cleaned_content = doc.page_content
        cleaned_content = fix_common_ocr_errors(cleaned_content)
        cleaned_content = normalize_text(cleaned_content)
        cleaned_content = remove_non_alphanumeric_and_normalize_space(cleaned_content)
        doc.page_content = cleaned_content
        processed_docs.append(doc)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(processed_docs)

    if not splits:
        st.warning("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c n·ªôi dung n√†o t·ª´ t√†i li·ªáu sau khi chia nh·ªè.")
        return None

    vector_store = FAISS.from_documents(splits, embeddings_obj)
    return vector_store

# --- Menu ƒëi·ªÅu h∆∞·ªõng d·ªçc ·ªü Sidebar ---
with st.sidebar:
    selected = option_menu(
        menu_title=None,
        options=["Chatbot", "Qu·∫£n l√Ω d·ªØ li·ªáu"],
        icons=["chat", "file-earmark-text"],
        menu_icon="list",
        default_index=0,
        orientation="vertical",
        styles={
            "container": {"padding": "0!important", "background-color": "transparent"},
            "icon": {"color": "orange", "font-size": "16px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#2ca8f2"},
        }
    )

# --- Logic hi·ªÉn th·ªã n·ªôi dung d·ª±a tr√™n l·ª±a ch·ªçn menu ---
if selected == "Chatbot":
    st.title("ü§ñ Chatbot")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                if st.session_state.vector_store:
                    retriever = st.session_state.vector_store.as_retriever()
                    # S·ª≠ d·ª•ng rag_prompt (ChatPromptTemplate) ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n
                    combine_docs_chain = create_stuff_documents_chain(chat, rag_prompt)
                    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

                    response = retrieval_chain.invoke({"input": prompt})
                    answer = response["answer"]

                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu c√° nh√¢n. Chatbot s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                    fallback_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† t·ª± nhi√™n:
C√¢u h·ªèi: {input}
"""
                    # S·ª≠ d·ª•ng ChatPromptTemplate cho fallback c≈©ng v·∫≠y
                    default_prompt = ChatPromptTemplate.from_messages([
                        ("system", fallback_prompt_template),
                        ("human", "{input}")
                    ])
                    chain = default_prompt | chat
                    response = chain.invoke({"input": prompt})
                    answer = response.content

                with st.chat_message("assistant"):
                    st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

elif selected == "Qu·∫£n l√Ω d·ªØ li·ªáu":
    st.header("Trang Qu·∫£n l√Ω D·ªØ li·ªáu")
    st.write("T·∫°i ƒë√¢y, b·∫°n c√≥ th·ªÉ t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho chatbot ho·∫∑c xem tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán c√≥.")

    if embeddings is None:
        st.warning("M√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt.")

    uploaded_file = st.file_uploader(
        "Ch·ªçn m·ªôt file PDF, DOCX ho·∫∑c XLSX ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu",
        type=["pdf", "docx", "xlsx"],
        accept_multiple_files=False,
        help="T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ƒë·ªÉ chatbot c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ƒë√≥. Vi·ªác t·∫£i file m·ªõi s·∫Ω ghi ƒë√® d·ªØ li·ªáu c≈©."
    )

    if 'file_uploaded' not in st.session_state:
        st.session_state.file_uploaded = False

    if 'last_processed_file_info' not in st.session_state:
        st.session_state.last_processed_file_info = None

    if uploaded_file is not None:
        current_file_info = (uploaded_file.name, uploaded_file.size)
        if current_file_info != st.session_state.last_processed_file_info:
            if embeddings is None:
                st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√¨ m√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói tr√™n c√πng.")
            else:
                import tempfile
                import shutil
                temp_dir = tempfile.mkdtemp()
                temp_file_path = os.path.join(temp_dir, uploaded_file.name)

                with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
                    try:
                        file_extension = uploaded_file.name.split(".")[-1].lower()

                        with open(temp_file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                        new_vector_store = process_document(temp_file_path, file_extension, embeddings)

                        if new_vector_store:
                            st.session_state.vector_store = new_vector_store
                            st.session_state.vector_store.save_local(FAISS_PATH)

                            st.success(f"T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† c·∫≠p nh·∫≠t.")
                            st.session_state.last_processed_file_info = current_file_info
                            st.session_state.file_uploaded = True

                            # Reset c√°c c·ªù tr·∫°ng th√°i ƒë·ªÉ th√¥ng b√°o FAISS c√≥ th·ªÉ hi·ªÉn th·ªã l·∫°i n·∫øu c·∫ßn
                            st.session_state.initial_faiss_loaded_toast_shown = False
                            st.session_state.initial_faiss_not_found_toast_shown = False
                            st.session_state.initial_faiss_error_toast_shown = False
                            st.session_state.initial_faiss_load_attempted = False
                        else:
                            st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu ho·∫∑c kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c n·ªôi dung.")
                    except Exception as e:
                        st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}. Vui l√≤ng ki·ªÉm tra file ho·∫∑c c√†i ƒë·∫∑t!")
                    finally:
                        # D·ªçn d·∫πp: X√≥a th∆∞ m·ª•c t·∫°m th·ªùi
                        if os.path.exists(temp_dir):
                            shutil.rmtree(temp_dir)
        else:
            st.session_state.last_processed_file_info = None
    else:
        st.session_state.file_uploaded = False

    st.markdown("---")
    st.subheader("Tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán t·∫°i:")
    if st.session_state.vector_store:
        st.write("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i v√† s·∫µn s√†ng s·ª≠ d·ª•ng trong Chatbot.")
    else:
        st.write("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫£i. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.")

    # if st.button("X√≥a FAISS Index hi·ªán c√≥"):
    #     if os.path.exists(FAISS_PATH):
    #         import shutil
    #         try:
    #             shutil.rmtree(FAISS_PATH)
    #             st.session_state.vector_store = None
    #             st.session_state.initial_faiss_loaded_toast_shown = False
    #             st.session_state.initial_faiss_not_found_toast_shown = False
    #             st.session_state.initial_faiss_error_toast_shown = False
    #             st.session_state.initial_faiss_load_attempted = False
    #             st.success("FAISS index ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng. Vui l√≤ng t·∫£i l·∫°i trang ho·∫∑c t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ t·∫°o l·∫°i.")
    #             st.rerun()
    #         except Exception as e:
    #             st.error(f"L·ªói khi x√≥a FAISS index: {e}")
    #     else:
    #         st.info("Kh√¥ng t√¨m th·∫•y FAISS index ƒë·ªÉ x√≥a.")
    st.subheader("Ki·ªÉm tra t√†i li·ªáu")
    st.write("Nh·∫•n n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra t·ª± ƒë·ªông")

    if 'docs_check_result' not in st.session_state:
        st.session_state.docs_check_result = ""
    
    if st.session_state.vector_store:
        if st.button("Ki·ªÉm tra", key="check_button"):
            with st.spinner("ƒêang th·ª±c hi·ªán ki·ªÉm tra..."):
                result_for_button = handleCheck(
                    chat,
                    st.session_state.vector_store
                )
                st.session_state.docs_check_result = result_for_button
    else:
        st.warning("Vui l√≤ng t·∫£i l√™n t·ªáp t√†i li·ªáu tr∆∞·ªõc khi th·ª±c hi·ªán ki·ªÉm tra.")

    
    if st.session_state.docs_check_result:
        columns = ["STT", "T√™n vƒÉn b·∫£n", "Ng√†y ph√°t h√†nh", "Tr·∫°ng th√°i"]
        df = pd.DataFrame(st.session_state.docs_check_result, columns=columns)
        
        st.dataframe(df, use_container_width=True)
    else:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë·ªÉ hi·ªÉn th·ªã.")