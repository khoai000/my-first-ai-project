import os
import streamlit as st
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from streamlit_option_menu import option_menu
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
import pandas as pd
from datetime import datetime
import math
import re

load_dotenv()

st.set_page_config(page_title="ü§ñ Chatbot AI", layout="wide")

genai_api_key = os.getenv('GOOGLE_API_KEY')
if not genai_api_key:
    st.error("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env. Vui l√≤ng th√™m kh√≥a API c·ªßa Gemini.")
    st.stop()

genai.configure(api_key=genai_api_key)

chat = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    google_api_key=genai_api_key,
    convert_system_message_to_human=True
)
PROMPT_TEMPLATE = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.
    S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.

    Ng·ªØ c·∫£nh:
    {context}
    """
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", PROMPT_TEMPLATE),
    ("human", "{input}")
])


# --- Kh·ªüi t·∫°o Session State Flags cho th√¥ng b√°o ---
if 'initial_embed_toast_shown' not in st.session_state:
    st.session_state.initial_embed_toast_shown = False
if 'initial_embed_error_toast_shown' not in st.session_state:
    st.session_state.initial_embed_error_toast_shown = False
if 'initial_faiss_loaded_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_loaded_toast_shown = False
if 'initial_faiss_not_found_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_not_found_toast_shown = False
if 'initial_faiss_error_toast_shown' not in st.session_state:
    st.session_state.initial_faiss_error_toast_shown = False
if 'initial_faiss_load_attempted' not in st.session_state:
    st.session_state.initial_faiss_load_attempted = False


# --- ƒê·∫∂T ƒê∆Ø·ªúNG D·∫™N C·ª§C B·ªò ---
current_script_directory = os.path.dirname(os.path.abspath(__file__))
local_embedding_model_path = os.path.join(current_script_directory, "local_models", "multilingual-e5-large")
REMOTE_EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
# REMOTE_EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"


# --- H√†m t·∫£i Embedding Model (CH·ªà H√ÄM N√ÄY D√ôNG @st.cache_resource) ---
@st.cache_resource
def get_huggingface_embeddings_model(model_path: str, is_local: bool):
    try:
        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa model c·ª•c b·ªô n·∫øu is_local l√† True
        if is_local and not (os.path.exists(model_path) and
                             (os.path.exists(os.path.join(model_path, 'pytorch_model.bin')) or
                              os.path.exists(os.path.join(model_path, 'model.safetensors')) or
                              os.path.exists(os.path.join(model_path, 'config.json')))):
            return None, "load_error:Local model path provided but no valid model files found."

        embed_model = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={'device': 'cpu'}
        )
        return embed_model, "loaded_successfully"
    except Exception as e:
        return None, f"load_error:{e}"

# --- Logic t·∫£i m√¥ h√¨nh Embedding khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng ---
if "embeddings_object" not in st.session_state or st.session_state.embeddings_object is None:
    # 1. Th·ª≠ t·∫£i t·ª´ c·ª•c b·ªô tr∆∞·ªõc
    with st.spinner(f"ƒêang ki·ªÉm tra v√† t·∫£i m√¥ h√¨nh Embedding t·ª´ c·ª•c b·ªô ({local_embedding_model_path})..."):
        embed_model_result, status = get_huggingface_embeddings_model(local_embedding_model_path, is_local=True)

    if embed_model_result:
        st.session_state.embeddings_object = embed_model_result
        if not st.session_state.initial_embed_toast_shown:
            st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ c·ª•c b·ªô!", icon="‚úÖ")
            st.session_state.initial_embed_toast_shown = True
    else:
        # 2. Th·ª≠ t·∫£i t·ª´ Internet n·∫øu c·ª•c b·ªô kh√¥ng th√†nh c√¥ng
        with st.spinner(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh c·ª•c b·ªô. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})..."):
            st.toast(f"M√¥ h√¨nh Embedding c·ª•c b·ªô kh√¥ng t√¨m th·∫•y. ƒêang th·ª≠ t·∫£i t·ª´ Internet ({REMOTE_EMBEDDING_MODEL_NAME})...", icon="üåê")
            embed_model_result, status = get_huggingface_embeddings_model(REMOTE_EMBEDDING_MODEL_NAME, is_local=False)

        if embed_model_result:
            st.session_state.embeddings_object = embed_model_result
            if not st.session_state.initial_embed_toast_shown:
                st.toast(f"M√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ Internet!", icon="‚úÖ")
                st.session_state.initial_embed_toast_shown = True
        else:
            if not st.session_state.initial_embed_error_toast_shown:
                error_message = status.split(":", 1)[1] if ":" in status else status
                st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh Embedding: {error_message}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Internet ho·∫∑c ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh.")
                st.session_state.initial_embed_error_toast_shown = True
            st.stop()

embeddings = st.session_state.embeddings_object

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u FAISS index ---
FAISS_PATH = "faiss_index_data_multilingual"
if not os.path.exists(FAISS_PATH):
    os.makedirs(FAISS_PATH)

# --- Qu·∫£n l√Ω FAISS Vector Store trong session state ---
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "processing_done" not in st.session_state:
    st.session_state.processing_done = False


# --- H√†m t·∫£i FAISS index (KH√îNG D√ôNG @st.cache_resource) ---
# H√†m n√†y ch·ªâ ch·ª©a logic t·∫£i/ki·ªÉm tra, kh√¥ng c√≥ l·ªánh Streamlit UI
def load_faiss_index_from_disk(path, embeddings_obj):
    if embeddings_obj is None: # ƒê·∫£m b·∫£o embeddings ƒë√£ s·∫µn s√†ng
        return None, "embeddings_not_ready"

    if os.path.exists(path) and os.listdir(path):
        try:
            vector_store = FAISS.load_local(
                path,
                embeddings_obj, # S·ª≠ d·ª•ng ƒë·ªëi t∆∞·ª£ng embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i v√† cache
                allow_dangerous_deserialization=True
            )
            return vector_store, "loaded_successfully"
        except Exception as e:
            return None, f"load_error:{e}"
    else:
        return None, "not_found"



# Ch·ªâ c·ªë g·∫Øng t·∫£i FAISS t·ª´ ƒëƒ©a n·∫øu n√≥ ch∆∞a ƒë∆∞·ª£c t·∫£i v√†o session_state V√Ä ch∆∞a t·ª´ng th·ª≠ t·∫£i
if st.session_state.vector_store is None and not st.session_state.initial_faiss_load_attempted:
    with st.spinner(f"ƒêang t·∫£i d·ªØ li·ªáu AI t·ª´ FAISS index ƒë√£ l∆∞u trong '{FAISS_PATH}'..."):
        st.session_state.vector_store, status = load_faiss_index_from_disk(FAISS_PATH, embeddings) # Truy·ªÅn embeddings ƒë√£ ƒë∆∞·ª£c cache

    if status == "loaded_successfully":
        if not st.session_state.initial_faiss_loaded_toast_shown:
            st.toast(f"FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ th∆∞ m·ª•c '{FAISS_PATH}'.", icon="‚úÖ")
            st.session_state.initial_faiss_loaded_toast_shown = True
        st.session_state.processing_done = True
    elif status.startswith("load_error"):
        error_message = status.split(":", 1)[1]
        if not st.session_state.initial_faiss_error_toast_shown:
            st.error(f"L·ªói khi t·∫£i FAISS index t·ª´ c·ª•c b·ªô: {error_message}. Vui l√≤ng th·ª≠ t·∫£i l·∫°i t√†i li·ªáu.")
            st.session_state.initial_faiss_error_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "not_found":
        if not st.session_state.initial_faiss_not_found_toast_shown:
            st.toast(f"Ch∆∞a t√¨m th·∫•y FAISS index trong th∆∞ m·ª•c '{FAISS_PATH}'. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.", icon="‚ÑπÔ∏è")
            st.session_state.initial_faiss_not_found_toast_shown = True
        st.session_state.processing_done = False
        st.session_state.vector_store = None
    elif status == "embeddings_not_ready":
        # ƒêi·ªÅu n√†y s·∫Ω kh√¥ng x·∫£y ra n·∫øu logic t·∫£i embeddings ch·∫°y tr∆∞·ªõc
        pass
    st.session_state.initial_faiss_load_attempted = True # ƒê√°nh d·∫•u l√† ƒë√£ th·ª≠ t·∫£i l·∫ßn ƒë·∫ßu

def read_excel_to_array():
    file_path = "./data/1. AI . DM n·ªôi dung tr√¨nh HƒêTV t·ª´ Ban K·∫ø ho·∫°ch (B02) final.xlsx"
    sheet_name = "Danh s√°ch vƒÉn b·∫£n"
    try:
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
        else:
            df = pd.read_excel(file_path)
        data_array = df.values.tolist()
        return data_array
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n '{file_path}'")
        return None
    except KeyError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y sheet '{sheet_name}' trong file.")
        return None
    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói khi ƒë·ªçc file Excel: {e}")
        return None
    
def handleCheck(llm_model, vector_store):
    data_from_excel = read_excel_to_array()
    if vector_store:
        try:
            retriever = vector_store.as_retriever()
            check_prompt_template = """
                B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v√† t·ªâ m·ªâ.

                **Nhi·ªám v·ª•:**
                T·ª´ ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi, h√£y **t√¨m v√† li·ªát k√™ ch√≠nh x√°c T·∫§T C·∫¢ c√°c d√≤ng ho·∫∑c ƒëo·∫°n vƒÉn b·∫£n** m√† trong ƒë√≥ xu·∫•t hi·ªán t·ª´ ho·∫∑c c·ª•m t·ª´ **'cƒÉn c·ª©'**.

                **L∆∞u √Ω quan tr·ªçng:**
                * H√£y ch√∫ √Ω ƒë·∫øn **ng·ªØ c·∫£nh v√† c·∫•u tr√∫c c√¢u** ƒë·ªÉ ƒë·∫£m b·∫£o 'cƒÉn c·ª©' ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë√∫ng nghƒ©a.
                * **Tr√°nh c√°c l·ªói ƒë·ªãnh d·∫°ng ho·∫∑c k√Ω t·ª± l·∫°** khi tr√≠ch xu·∫•t. Ch·ªâ tr·∫£ v·ªÅ ph·∫ßn vƒÉn b·∫£n g·ªëc, chu·∫©n x√°c.
                * N·∫øu m·ªôt d√≤ng/ƒëo·∫°n c√≥ ch·ª©a 'cƒÉn c·ª©' v√† sau ƒë√≥ l·∫°i b·ªã s·ª≠a ƒë·ªïi ho·∫∑c h·ªßy b·ªè b·ªüi m·ªôt ghi ch√∫, b·∫°n v·∫´n li·ªát k√™ n√≥ nh∆∞ng c√≥ th·ªÉ ghi ch√∫ th√™m n·∫øu th√¥ng tin ƒë√≥ r√µ r√†ng trong ng·ªØ c·∫£nh.

                **ƒê·ªãnh d·∫°ng k·∫øt qu·∫£:**
                Li·ªát k√™ m·ªói d√≤ng/ƒëo·∫°n t√¨m ƒë∆∞·ª£c tr√™n m·ªôt d√≤ng ri√™ng.
                N·∫øu kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ d√≤ng/ƒëo·∫°n n√†o ch·ª©a 'cƒÉn c·ª©', h√£y tr·∫£ l·ªùi r√µ r√†ng: "Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ d·ªØ li·ªáu trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p."

                **Ng·ªØ c·∫£nh:**
                {context}
                """
            check_rag_prompt = ChatPromptTemplate.from_messages([
                ("system", check_prompt_template),
                ("human", "{input}")
            ])

            combine_docs_chain = create_stuff_documents_chain(llm_model, check_rag_prompt)
            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

            response = retrieval_chain.invoke({"input": "T√¨m nh·ªØng d√≤ng c√≥ t·ª´ 'cƒÉn c·ª©' trong vƒÉn b·∫£n"})

            results = []
            date_pattern = re.compile(r'(\d{1,2})/(\d{1,2})/(\d{4})')
            for item in data_from_excel:
                stt = item[0] 
                doc_title = item[1] 
                found_date_str_in_title = None 
                status = item[3]

                if isinstance(doc_title, str):
                    match_obj = date_pattern.search(doc_title)
                    
                    if match_obj:
                        found_date_str_in_title = match_obj.group(0)
                text_matches = isinstance(doc_title, str) and doc_title.lower() in response["answer"].lower()
                date_matches = False
                if found_date_str_in_title:
                    date_matches = found_date_str_in_title in response["answer"] 

                if text_matches or date_matches:
                    new_record = [stt, doc_title, found_date_str_in_title if found_date_str_in_title is not None else "", status if status is not None else ""]
                    results.append(new_record)
            return results
        except Exception as e:
            return f"L·ªói khi th·ª±c hi·ªán ki·ªÉm tra: {str(e)}"
    else:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu t·∫£i l√™n ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu tr∆∞·ªõc."


# --- Menu ƒëi·ªÅu h∆∞·ªõng d·ªçc ·ªü Sidebar ---
with st.sidebar:
    selected = option_menu(
        menu_title=None,
        options=["Chatbot", "Qu·∫£n l√Ω d·ªØ li·ªáu"],
        icons=["chat", "file-earmark-text"],
        menu_icon="list",
        default_index=0,
        orientation="vertical",
        styles={
            "container": {"padding": "0!important", "background-color": "transparent"},
            "icon": {"color": "orange", "font-size": "16px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#2ca8f2"},
        }
    )

# --- Logic hi·ªÉn th·ªã n·ªôi dung d·ª±a tr√™n l·ª±a ch·ªçn menu ---
if selected == "Chatbot":
    st.title("ü§ñ Chatbot")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                if st.session_state.vector_store:
                    retriever = st.session_state.vector_store.as_retriever()
                    # S·ª≠ d·ª•ng rag_prompt (ChatPromptTemplate) ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n
                    combine_docs_chain = create_stuff_documents_chain(chat, rag_prompt)
                    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

                    response = retrieval_chain.invoke({"input": prompt})
                    answer = response["answer"]

                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu c√° nh√¢n. Chatbot s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                    fallback_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng, ch√≠nh x√°c v√† t·ª± nhi√™n:
C√¢u h·ªèi: {input}
"""
                    # S·ª≠ d·ª•ng ChatPromptTemplate cho fallback c≈©ng v·∫≠y
                    default_prompt = ChatPromptTemplate.from_messages([
                        ("system", fallback_prompt_template),
                        ("human", "{input}")
                    ])
                    chain = default_prompt | chat
                    response = chain.invoke({"input": prompt})
                    answer = response.content

                with st.chat_message("assistant"):
                    st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

elif selected == "Qu·∫£n l√Ω d·ªØ li·ªáu":
    st.header("Trang Qu·∫£n l√Ω D·ªØ li·ªáu")
    st.write("T·∫°i ƒë√¢y, b·∫°n c√≥ th·ªÉ t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho chatbot ho·∫∑c xem tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán c√≥.")

    if embeddings is None:
        st.warning("M√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt chi ti·∫øt.")

    uploaded_file = st.file_uploader(
        "Ch·ªçn m·ªôt file PDF, DOCX ho·∫∑c XLSX ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu",
        type=["pdf", "docx", "xlsx"],
        accept_multiple_files=False,
        help="T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ƒë·ªÉ chatbot c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ƒë√≥. Vi·ªác t·∫£i file m·ªõi s·∫Ω ghi ƒë√® d·ªØ li·ªáu c≈©."
    )

    if 'last_processed_file_info' not in st.session_state:
        st.session_state.last_processed_file_info = None

    if uploaded_file:
        current_file_info = (uploaded_file.name, uploaded_file.size)
        if current_file_info != st.session_state.last_processed_file_info:
            if embeddings is None:
                st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu v√¨ m√¥ h√¨nh Embedding kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c√°c th√¥ng b√°o l·ªói tr√™n c√πng.")
            else:
                # T·∫°o m·ªôt th∆∞ m·ª•c t·∫°m th·ªùi ƒë·ªÉ l∆∞u file
                import tempfile
                import shutil
                temp_dir = tempfile.mkdtemp()
                temp_file_path = os.path.join(temp_dir, uploaded_file.name)

                with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
                    try:
                        file_extension = uploaded_file.name.split(".")[-1].lower()

                        with open(temp_file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                        docs = []
                        if file_extension == "pdf":
                            loader = PyPDFLoader(temp_file_path)
                            docs = loader.load()
                        elif file_extension == "docx":
                            loader = UnstructuredWordDocumentLoader(temp_file_path)
                            docs = loader.load()
                        elif file_extension == "xlsx":
                            loader = UnstructuredExcelLoader(temp_file_path)
                            docs = loader.load()
                        else:
                            st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i l√™n file PDF, DOCX, ho·∫∑c XLSX.")
                            shutil.rmtree(temp_dir)
                            st.stop()

                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=1000,
                            chunk_overlap=200
                        )
                        splits = text_splitter.split_documents(docs)

                        # Lu√¥n t·∫°o m·ªõi ho·∫∑c ghi ƒë√® FAISS index khi t·∫£i file m·ªõi
                        st.session_state.vector_store = FAISS.from_documents(splits, embeddings)
                        st.session_state.vector_store.save_local(FAISS_PATH)

                        st.success(f"T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.")
                        st.session_state.last_processed_file_info = current_file_info

                        # Reset c√°c c·ªù tr·∫°ng th√°i ƒë·ªÉ th√¥ng b√°o FAISS c√≥ th·ªÉ hi·ªÉn th·ªã l·∫°i n·∫øu c·∫ßn
                        st.session_state.initial_faiss_loaded_toast_shown = False
                        st.session_state.initial_faiss_not_found_toast_shown = False
                        st.session_state.initial_faiss_error_toast_shown = False
                        st.session_state.initial_faiss_load_attempted = False # Bu·ªôc t·∫£i l·∫°i FAISS t·ª´ ƒëƒ©a n·∫øu trang ƒë∆∞·ª£c l√†m m·ªõi

                    except Exception as e:
                        st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}. Vui l√≤ng ki·ªÉm tra file ho·∫∑c c√†i ƒë·∫∑t!")
                    finally:
                        # D·ªçn d·∫πp: X√≥a th∆∞ m·ª•c t·∫°m th·ªùi
                        if os.path.exists(temp_dir):
                            shutil.rmtree(temp_dir)
        else:
            st.session_state.last_processed_file_info = None

    st.markdown("---")
    st.subheader("Tr·∫°ng th√°i d·ªØ li·ªáu hi·ªán t·∫°i:")
    if st.session_state.vector_store:
        st.write("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i v√† s·∫µn s√†ng s·ª≠ d·ª•ng trong Chatbot.")
    else:
        st.write("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫£i. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu.")

    # if st.button("X√≥a FAISS Index hi·ªán c√≥"):
    #     if os.path.exists(FAISS_PATH):
    #         import shutil
    #         try:
    #             shutil.rmtree(FAISS_PATH)
    #             st.session_state.vector_store = None
    #             st.session_state.initial_faiss_loaded_toast_shown = False
    #             st.session_state.initial_faiss_not_found_toast_shown = False
    #             st.session_state.initial_faiss_error_toast_shown = False
    #             st.session_state.initial_faiss_load_attempted = False
    #             st.success("FAISS index ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng. Vui l√≤ng t·∫£i l·∫°i trang ho·∫∑c t·∫£i l√™n t√†i li·ªáu m·ªõi ƒë·ªÉ t·∫°o l·∫°i.")
    #             st.rerun()
    #         except Exception as e:
    #             st.error(f"L·ªói khi x√≥a FAISS index: {e}")
    #     else:
    #         st.info("Kh√¥ng t√¨m th·∫•y FAISS index ƒë·ªÉ x√≥a.")
    st.subheader("Ki·ªÉm tra t√†i li·ªáu")
    st.write("Nh·∫•n n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra t·ª± ƒë·ªông")

    if 'docs_check_result' not in st.session_state:
        st.session_state.docs_check_result = ""
    
    if st.button("Th·ª±c hi·ªán Ki·ªÉm tra"):
        canchu_check_result_placeholder = st.empty()
        with st.spinner("ƒêang th·ª±c hi·ªán ki·ªÉm tra"):
            result_for_button = handleCheck(
                chat,
                st.session_state.vector_store
            )
            st.session_state.docs_check_result = result_for_button
    
    if st.session_state.docs_check_result:
        columns = ["STT", "T√™n vƒÉn b·∫£n", "Ng√†y ph√°t h√†nh", "Tr·∫°ng th√°i"]
        df = pd.DataFrame(st.session_state.docs_check_result, columns=columns)
        
        st.dataframe(df, use_container_width=True)
    else:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë·ªÉ hi·ªÉn th·ªã.")